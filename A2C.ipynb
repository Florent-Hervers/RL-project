{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72d2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62bcd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "CONFIG_NUMBER = 13\n",
    "\n",
    "OBSERVATION_SIZE = 64\n",
    "NB_FRAMES = 1\n",
    "NB_ENVS = 8\n",
    "CUDA = False\n",
    "SEED = 2307\n",
    "\n",
    "MAX_EPISODE_LENGTH = 12000\n",
    "LEARNING_RATE = 1e-4\n",
    "NB_STEPS = 2048\n",
    "TOTAL_TIMESTEPS = 2e6\n",
    "RUN_NAME = \"A2C with GAE\"\n",
    "LR_SCHEDULING = \"Linear\"\n",
    "GAMMA = 0.99\n",
    "VF_COEF = 0.5\n",
    "ENT_COEF = 0.1\n",
    "MAX_GRAD_NORM = 0.5\n",
    "GAE_LAMBDA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73a7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def make_env():\n",
    "    def thunk():\n",
    "        env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.95, domain_randomize=False, continuous=True, max_episode_steps=MAX_EPISODE_LENGTH)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (OBSERVATION_SIZE, OBSERVATION_SIZE))\n",
    "        env = gym.wrappers.GrayscaleObservation(env)\n",
    "        env = gym.wrappers.FrameStackObservation(env, NB_FRAMES)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv([make_env() for _ in range(NB_ENVS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9867dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from TP5\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, nb_frames, image_size):\n",
    "        super(Agent, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.nb_frames = nb_frames\n",
    "\n",
    "        # Actor network\n",
    "        self.actor_network = self.build_network()\n",
    "        self.actor_mean = layer_init(\n",
    "            nn.Linear(512, np.prod(envs.single_action_space.shape)),\n",
    "            std=0.01\n",
    "        )\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))\n",
    "\n",
    "        # Critic network\n",
    "        self.critic_network = self.build_network()\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "\n",
    "    def build_network(self):\n",
    "        stride = [4, 2, 1]\n",
    "        kernel_size = [8, 4, 3]\n",
    "        input_channels = [self.nb_frames, 32, 64]\n",
    "        output_channels = [32, 64, 64]\n",
    "        image_size = self.image_size\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(stride)):\n",
    "            layers.append(layer_init(nn.Conv2d(input_channels[i], output_channels[i], kernel_size[i], stride=stride[i])))\n",
    "            layers.append(nn.ReLU())\n",
    "            image_size = math.floor(((image_size - kernel_size[i]) / stride[i]) + 1)\n",
    "\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(layer_init(nn.Linear(output_channels[-1] * image_size * image_size, 512)))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        hidden = self.critic_network(x / 255.0)\n",
    "        return self.critic(hidden)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        actor_hidden = self.actor_network(x / 255.0)\n",
    "        action_mean = self.actor_mean(actor_hidden)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        critic_hidden = self.critic_network(x / 255.0)\n",
    "        value = self.critic(critic_hidden)\n",
    "\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468837cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflo230702\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.5s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Session/Documents/Universite/Master 2/Reinforcement learning/Project/wandb/run-20250508_154456-jea8q1z4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/jea8q1z4' target=\"_blank\">A2C with GAE</a></strong> to <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/jea8q1z4' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project/runs/jea8q1z4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and CUDA else \"cpu\")\n",
    "agent = Agent(envs, NB_FRAMES, OBSERVATION_SIZE).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "obs = torch.zeros((NB_STEPS, NB_ENVS) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((NB_STEPS, NB_ENVS) + envs.single_action_space.shape).to(device)\n",
    "rewards = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "values = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "current_logprobs = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "previous_logprobs = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "\n",
    "global_step = 0\n",
    "next_obs, _ = envs.reset(seed=SEED)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(NB_ENVS).to(device)\n",
    "\n",
    "if RUN_NAME is not None:\n",
    "    import wandb\n",
    "\n",
    "    config = {\n",
    "        \"OBSERVATION_SIZE\": OBSERVATION_SIZE,\n",
    "        \"NB_FRAMES\": NB_FRAMES,\n",
    "        \"NB_ENVS\": NB_ENVS,\n",
    "        \"SEED\": SEED,\n",
    "        \"MAX_EPISODE_LENGTH\": MAX_EPISODE_LENGTH,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"NB_STEPS\": NB_STEPS,\n",
    "        \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,\n",
    "        \"LR_SCHEDULING\": LR_SCHEDULING,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"VF_COEF\": VF_COEF,\n",
    "        \"CUDA\": CUDA,\n",
    "        \"ENT_COEF\": ENT_COEF,\n",
    "        \"CONFIG_NUMBER\": CONFIG_NUMBER,\n",
    "        \"GAE_LAMBDA\": GAE_LAMBDA\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        entity=\"Rl2025-project\",\n",
    "        project=\"RL Project\",\n",
    "        name=RUN_NAME,\n",
    "        config=config,\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "        monitor_gym=True,       # auto-upload des vidéos de l'agent\n",
    "        # save_code=True,       # optionnel\n",
    "    )\n",
    "    \n",
    "    writer = SummaryWriter(f\"runs/{RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceadde90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=63376, episodic_return=-787.0275362319696\n",
      "global_step=91944, episodic_return=-1189.709915014268\n",
      "global_step=94800, episodic_return=-1214.477464788814\n",
      "global_step=96000, episodic_return=-1120.5776173285951\n",
      "global_step=96000, episodic_return=-1120.1277955272317\n",
      "global_step=96000, episodic_return=-1121.7687074830676\n",
      "global_step=96000, episodic_return=-1124.731182795771\n",
      "global_step=96000, episodic_return=-1129.0322580645836\n",
      "global_step=159384, episodic_return=-1130.091185410401\n",
      "global_step=187952, episodic_return=-1075.7961783440437\n",
      "global_step=190808, episodic_return=-1149.3670886076397\n",
      "global_step=192008, episodic_return=-963.440860215229\n",
      "global_step=192008, episodic_return=-1015.4362416109087\n",
      "global_step=192008, episodic_return=-1122.0338983051604\n",
      "global_step=192008, episodic_return=-1115.7509157509962\n",
      "global_step=192008, episodic_return=-1029.5454545456253\n",
      "global_step=255392, episodic_return=-1113.907284768299\n",
      "global_step=283960, episodic_return=-975.4385964914164\n",
      "global_step=286816, episodic_return=-1000.0000000001604\n",
      "global_step=288016, episodic_return=-1057.1428571430051\n",
      "global_step=288016, episodic_return=-1027.5862068967194\n",
      "global_step=288016, episodic_return=-991.6666666668347\n",
      "global_step=288016, episodic_return=-1021.548821549012\n",
      "global_step=288016, episodic_return=-1131.034482758695\n",
      "global_step=351400, episodic_return=-1097.1061093248677\n",
      "global_step=379968, episodic_return=-1024.1042345278815\n",
      "global_step=382824, episodic_return=-1084.6153846154946\n",
      "global_step=384024, episodic_return=-1078.47222222235\n",
      "global_step=384024, episodic_return=-1085.906040268582\n",
      "global_step=384024, episodic_return=-1050.1742160280405\n",
      "global_step=384024, episodic_return=-1099.3506493507573\n",
      "global_step=384024, episodic_return=-1062.7450980393592\n",
      "global_step=447408, episodic_return=-1066.8831168832662\n",
      "global_step=475976, episodic_return=-1015.3310104531471\n",
      "global_step=478832, episodic_return=-934.375000000188\n",
      "global_step=480032, episodic_return=-1076.5822784811483\n",
      "global_step=480032, episodic_return=-1089.9082568808478\n",
      "global_step=480032, episodic_return=-1032.2147651008377\n",
      "global_step=480032, episodic_return=-1082.8828828830005\n",
      "global_step=480032, episodic_return=-1043.7500000001749\n",
      "global_step=543416, episodic_return=-1119.0751445087549\n",
      "global_step=571984, episodic_return=-1066.4383561645207\n",
      "global_step=574840, episodic_return=-1059.848484848638\n",
      "global_step=576040, episodic_return=-1104.1095890411962\n",
      "global_step=576040, episodic_return=-1042.62295081981\n",
      "global_step=576040, episodic_return=-1088.4892086332065\n",
      "global_step=576040, episodic_return=-1057.1428571430042\n",
      "global_step=576040, episodic_return=-1034.024896265746\n",
      "global_step=639424, episodic_return=-1050.5338078293362\n",
      "global_step=667992, episodic_return=-1089.2508143323475\n",
      "global_step=670848, episodic_return=-1160.8540925267323\n",
      "global_step=672048, episodic_return=-1113.4328358209827\n",
      "global_step=672048, episodic_return=-1066.4259927799128\n",
      "global_step=672048, episodic_return=-1125.1968503937744\n",
      "global_step=672048, episodic_return=-1037.288135593395\n",
      "global_step=672048, episodic_return=-1055.5133079849502\n",
      "global_step=735432, episodic_return=-1073.6059479555154\n",
      "global_step=764000, episodic_return=-1080.3986710964746\n",
      "global_step=766856, episodic_return=-1131.159420289927\n",
      "global_step=768056, episodic_return=-1121.3114754099138\n",
      "global_step=768056, episodic_return=-1084.8684210527485\n",
      "global_step=768056, episodic_return=-1101.6949152543293\n",
      "global_step=768056, episodic_return=-1080.645161290435\n",
      "global_step=768056, episodic_return=-1093.772893773009\n",
      "global_step=831440, episodic_return=-1145.848375451316\n",
      "global_step=860008, episodic_return=-1062.416107382688\n",
      "global_step=862864, episodic_return=-1031.4606741574796\n",
      "global_step=864064, episodic_return=-1136.842105263218\n",
      "global_step=864064, episodic_return=-1110.8280254778008\n",
      "global_step=864064, episodic_return=-1026.2411347519644\n",
      "global_step=864064, episodic_return=-1090.6752411576613\n",
      "global_step=864064, episodic_return=-1044.4444444446117\n",
      "global_step=915888, episodic_return=-691.2201780416088\n",
      "global_step=927448, episodic_return=-1095.470383275379\n",
      "global_step=956016, episodic_return=-1004.4280442806164\n",
      "global_step=958872, episodic_return=-1011.1888111889941\n",
      "global_step=960072, episodic_return=-1118.9189189189997\n",
      "global_step=960072, episodic_return=-1076.0000000001291\n",
      "global_step=960072, episodic_return=-1064.1114982579738\n",
      "global_step=960072, episodic_return=-1095.131086142426\n",
      "global_step=1011896, episodic_return=-1090.7849829352765\n",
      "global_step=1023456, episodic_return=-1066.4383561645034\n",
      "global_step=1038952, episodic_return=-1045.036585366015\n",
      "global_step=1054880, episodic_return=-984.7222222224086\n",
      "global_step=1056080, episodic_return=-1138.6281588448303\n",
      "global_step=1056080, episodic_return=-1008.7649402392226\n",
      "global_step=1056080, episodic_return=-886.4111498259829\n",
      "global_step=1056080, episodic_return=-1072.964169381225\n",
      "global_step=1059312, episodic_return=-291.5937293729304\n",
      "global_step=1065152, episodic_return=-153.2000000000005\n",
      "global_step=1084184, episodic_return=-375.7989966555089\n",
      "global_step=1087952, episodic_return=-420.9901840490843\n",
      "global_step=1094736, episodic_return=-293.12941176469985\n",
      "global_step=1101240, episodic_return=-513.220863309379\n",
      "global_step=1107904, episodic_return=-761.5384615386454\n",
      "global_step=1119464, episodic_return=-1071.9512195123227\n",
      "global_step=1126296, episodic_return=-564.766552901061\n",
      "global_step=1142128, episodic_return=-731.5454545455286\n",
      "global_step=1143208, episodic_return=-536.635251798612\n",
      "global_step=1150056, episodic_return=-980.5702875401098\n",
      "global_step=1150880, episodic_return=-284.423420074338\n",
      "global_step=1152088, episodic_return=-1102.8213166145224\n",
      "global_step=1154096, episodic_return=-207.05384615384395\n",
      "global_step=1161184, episodic_return=-142.12580645161376\n",
      "global_step=1163464, episodic_return=-219.07718631178454\n",
      "global_step=1169672, episodic_return=-174.61858736059628\n",
      "global_step=1173304, episodic_return=-98.098550724638\n",
      "global_step=1178544, episodic_return=-321.4839160839082\n",
      "global_step=1181344, episodic_return=-96.47247386759592\n",
      "global_step=1183024, episodic_return=-62.168078175895985\n",
      "global_step=1188528, episodic_return=-906.9421052632822\n",
      "global_step=1191104, episodic_return=-307.13333333332423\n",
      "global_step=1191792, episodic_return=-322.69432624112517\n",
      "global_step=1192616, episodic_return=-936.8051948053558\n",
      "global_step=1194632, episodic_return=-109.92352941176486\n",
      "global_step=1194824, episodic_return=-251.58388278387852\n",
      "global_step=1201296, episodic_return=-146.992134831461\n",
      "global_step=1201336, episodic_return=-159.1311258278153\n",
      "global_step=1203120, episodic_return=-177.34306049822038\n",
      "global_step=1203952, episodic_return=-870.1857142858466\n",
      "global_step=1204032, episodic_return=-76.96289752650216\n",
      "global_step=1204672, episodic_return=-105.42618296530094\n",
      "global_step=1204720, episodic_return=-651.1393939394474\n",
      "global_step=1207496, episodic_return=-78.58387096774246\n",
      "global_step=1209104, episodic_return=-194.51830985915396\n",
      "global_step=1212768, episodic_return=-188.854258675078\n",
      "global_step=1213688, episodic_return=-115.34432989690745\n",
      "global_step=1214160, episodic_return=-355.24203821654737\n",
      "global_step=1215840, episodic_return=-158.4141479099689\n",
      "global_step=1217392, episodic_return=-108.00476190476188\n",
      "global_step=1222176, episodic_return=-94.26567164179197\n",
      "global_step=1222312, episodic_return=-129.35111821086286\n",
      "global_step=1223584, episodic_return=-126.94603174602547\n",
      "global_step=1225232, episodic_return=-147.2699300699308\n",
      "global_step=1226976, episodic_return=-108.86666666666676\n",
      "global_step=1229520, episodic_return=-248.82641509433014\n",
      "global_step=1229720, episodic_return=-206.3521172638426\n",
      "global_step=1229856, episodic_return=-159.19999999999885\n",
      "global_step=1231960, episodic_return=-90.63900709219877\n",
      "global_step=1233280, episodic_return=-186.87586206896515\n",
      "global_step=1236048, episodic_return=-110.00000000000097\n",
      "global_step=1236528, episodic_return=-89.75690235690188\n",
      "global_step=1238168, episodic_return=-177.63103448275902\n",
      "global_step=1243304, episodic_return=-164.7631578947374\n",
      "global_step=1244640, episodic_return=-109.83385826771719\n",
      "global_step=1244792, episodic_return=-104.71219512195259\n",
      "global_step=1246528, episodic_return=-421.0769230769258\n",
      "global_step=1247136, episodic_return=-79.27793594306084\n",
      "global_step=1250856, episodic_return=-258.6833333333267\n",
      "global_step=1253104, episodic_return=-159.0491803278694\n",
      "global_step=1255496, episodic_return=-108.86431535269764\n",
      "global_step=1266928, episodic_return=-250.87793594305592\n",
      "global_step=1267224, episodic_return=-183.69372937293514\n",
      "global_step=1269504, episodic_return=-338.9276450511851\n",
      "global_step=1270096, episodic_return=-80.1444444444449\n",
      "global_step=1273120, episodic_return=-453.30989010990294\n",
      "global_step=1275264, episodic_return=-612.3176656151903\n",
      "global_step=1277960, episodic_return=-441.2181494661977\n",
      "global_step=1290272, episodic_return=-321.49152542372065\n",
      "global_step=1335736, episodic_return=-900.3027397261408\n",
      "global_step=1339312, episodic_return=-894.0298507464507\n",
      "global_step=1349112, episodic_return=-1087.3720136519569\n",
      "global_step=1362936, episodic_return=-873.0158730160283\n",
      "global_step=1369128, episodic_return=-1092.8571428572454\n",
      "global_step=1371272, episodic_return=-1104.761904761984\n",
      "global_step=1373968, episodic_return=-1148.1481481481999\n",
      "global_step=1375128, episodic_return=-73.40139860139904\n",
      "global_step=1383576, episodic_return=-178.0027397260268\n",
      "global_step=1386280, episodic_return=-1144.6366782007467\n",
      "global_step=1400864, episodic_return=-698.312903225874\n",
      "global_step=1431744, episodic_return=-1138.3561643836224\n",
      "global_step=1435320, episodic_return=-1109.0909090910015\n",
      "global_step=1458944, episodic_return=-1169.8275862069281\n",
      "global_step=1467280, episodic_return=-1169.1780821918132\n",
      "global_step=1469976, episodic_return=-1120.2898550725374\n",
      "global_step=1479584, episodic_return=-1170.3703703704016\n",
      "global_step=1482288, episodic_return=-1121.6417910448524\n",
      "global_step=1496872, episodic_return=-1134.108527131847\n",
      "global_step=1527752, episodic_return=-1171.7314487632807\n",
      "global_step=1531328, episodic_return=-1094.2598187312192\n",
      "global_step=1554952, episodic_return=-1157.4468085106798\n",
      "global_step=1563288, episodic_return=-1183.1081081081277\n",
      "global_step=1565984, episodic_return=-1140.5940594060055\n",
      "global_step=1575592, episodic_return=-1166.7673716012473\n",
      "global_step=1578296, episodic_return=-1172.656250000029\n",
      "global_step=1592880, episodic_return=-1180.0000000000223\n",
      "global_step=1623760, episodic_return=-1183.8709677419545\n",
      "global_step=1627336, episodic_return=-1175.0000000000268\n",
      "global_step=1650960, episodic_return=-1176.9230769231021\n",
      "global_step=1659296, episodic_return=-1181.8181818182024\n",
      "global_step=1661992, episodic_return=-1163.5036496350738\n",
      "global_step=1671600, episodic_return=-1179.2387543252826\n",
      "global_step=1674304, episodic_return=-1152.2184300341837\n",
      "global_step=1688888, episodic_return=-1185.8657243816426\n",
      "global_step=1719768, episodic_return=-1170.9677419355144\n",
      "global_step=1723344, episodic_return=-1157.0552147239696\n",
      "global_step=1746968, episodic_return=-1177.346278317177\n",
      "global_step=1755304, episodic_return=-1181.3432835821106\n",
      "global_step=1758000, episodic_return=-1185.2507374631446\n",
      "global_step=1767608, episodic_return=-1181.1320754717194\n",
      "global_step=1770312, episodic_return=-1182.8767123287869\n",
      "global_step=1784896, episodic_return=-1169.2307692308016\n",
      "global_step=1815776, episodic_return=-1173.8805970149535\n",
      "global_step=1819352, episodic_return=-1182.7586206896751\n",
      "global_step=1842976, episodic_return=-1183.2214765100866\n",
      "global_step=1851312, episodic_return=-1185.0299401197783\n",
      "global_step=1854008, episodic_return=-1175.6097560975875\n",
      "global_step=1863616, episodic_return=-1182.5174825175027\n",
      "global_step=1866320, episodic_return=-1185.4651162790872\n",
      "global_step=1880904, episodic_return=-1182.0788530466154\n",
      "global_step=1911784, episodic_return=-1174.0740740741019\n",
      "global_step=1915360, episodic_return=-1175.1552795031325\n",
      "global_step=1938984, episodic_return=-1165.2509652510012\n",
      "global_step=1947320, episodic_return=-1178.947368421076\n",
      "global_step=1950016, episodic_return=-1180.1980198020024\n",
      "global_step=1959624, episodic_return=-1177.0773638968733\n",
      "global_step=1962328, episodic_return=-1182.2064056939705\n",
      "global_step=1976912, episodic_return=-1176.9230769231021\n",
      "global_step=2007792, episodic_return=-1182.638888888909\n",
      "Model saved to trained_models/a2c/a2c_config13.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>charts/episodic_length</td><td>▆██████████████▃█▂▁▁▁▃▁▄▄▆██▂███████████</td></tr><tr><td>charts/episodic_return</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄█▆▆██▇▆▅▆▂█▄▁▁▁▁▁▁▁▁▁</td></tr><tr><td>charts/learning_rate</td><td>█▇▇▇▇▇▆▆▆▆▆▅▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>losses/actor_loss</td><td>▁▁▁▁▃▁▁▁▁▃▁▁▁▃▁▂▁▃▂▁█▃▃▃▃▁▂▁▁▁▁▁▁▁▁▂▁▂▂▁</td></tr><tr><td>losses/approx_kl</td><td>▄▃█▄▃▃▅▄▄▃▇▂▄▅▃▃▅▂▄▄▄▄▃▃▃▁▅▃▄▄▅▃▂▃▆▄▃▆▁▃</td></tr><tr><td>losses/entropy</td><td>▁▁▁▁▂▂▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>losses/total_loss</td><td>▁▁▁▂▁▂▁▁▁▁▁▂▁▁▁▁▂▂▂▂▇▄▄▆▄█▆▂▂▃▁▁▂▁▁▁▂▂▂▁</td></tr><tr><td>losses/value_loss</td><td>▂▂▁▂▅▁▁▁▁▄▄▁▁▁▃▁▂▇▅▇▇█▅▅▅▂▆▁▄▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>charts/episodic_length</td><td>12000</td></tr><tr><td>charts/episodic_return</td><td>-1182.6389</td></tr><tr><td>charts/learning_rate</td><td>0.0</td></tr><tr><td>global_step</td><td>2007792</td></tr><tr><td>losses/actor_loss</td><td>0.1348</td></tr><tr><td>losses/approx_kl</td><td>-0.00923</td></tr><tr><td>losses/entropy</td><td>4.28095</td></tr><tr><td>losses/total_loss</td><td>-0.26249</td></tr><tr><td>losses/value_loss</td><td>0.06161</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">A2C with GAE</strong> at: <a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/jea8q1z4' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project/runs/jea8q1z4</a><br> View project at: <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250508_154456-jea8q1z4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "while global_step < TOTAL_TIMESTEPS:\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if LR_SCHEDULING == \"Linear\":\n",
    "        frac = 1.0 - global_step / TOTAL_TIMESTEPS\n",
    "        lrnow = frac * LEARNING_RATE\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, NB_STEPS):\n",
    "        global_step += NB_ENVS\n",
    "        obs[step] = next_obs\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "            current_logprobs[step] = logprob\n",
    "\n",
    "        actions[step] = action\n",
    "\n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "        \n",
    "        if \"episode\" in infos:\n",
    "            completed_episodes = infos[\"_episode\"]\n",
    "            episodic_returns = infos[\"episode\"][\"r\"][completed_episodes]\n",
    "            episodic_lengths = infos[\"episode\"][\"l\"][completed_episodes]\n",
    "\n",
    "            for episodic_return, episodic_length in zip(episodic_returns, episodic_lengths):\n",
    "                print(f\"global_step={global_step}, episodic_return={episodic_return}\")\n",
    "                if RUN_NAME != None:\n",
    "                    writer.add_scalar(\"charts/episodic_return\", episodic_return, global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", episodic_length, global_step)\n",
    "        \n",
    "        # Break when one of the environement as reached a terminal state\n",
    "        if torch.any(next_done):\n",
    "            break\n",
    "    \n",
    "    if GAE_LAMBDA == None:\n",
    "        R = torch.Tensor([0 if next_done[i] == True else values[-1][i] for i in range(len(next_done))]).to(device)\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "\n",
    "        for i in reversed(range(step)):\n",
    "            R = rewards[i] + GAMMA * R\n",
    "            returns[i] = R\n",
    "            advantages[i] = returns[i] - values[i]\n",
    "    \n",
    "    else:\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        lastgaelam = 0\n",
    "\n",
    "        for t in reversed(range(step)):\n",
    "            if t == step - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalue = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - torch.zeros_like(next_done)\n",
    "                nextvalue = values[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + GAMMA * nextvalue * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + GAMMA * GAE_LAMBDA * nextnonterminal * lastgaelam\n",
    "\n",
    "        returns = advantages + values\n",
    "\n",
    "    # Normalize the advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    value_loss = torch.zeros((step, NB_ENVS))\n",
    "    actor_loss = torch.zeros((step, NB_ENVS))\n",
    "    entropy_term = torch.zeros((step, NB_ENVS))\n",
    "\n",
    "    for i in range(step):\n",
    "        _, logprob, ent, value = agent.get_action_and_value(obs[i], actions[i])\n",
    "        value = value.flatten()\n",
    "\n",
    "        actor_loss[i] = -logprob * advantages[i]\n",
    "        value_loss[i] = (returns[i] - value)**2\n",
    "        entropy_term[i] = ent\n",
    "\n",
    "    actor_loss = actor_loss.mean()\n",
    "    value_loss = 0.5 * value_loss.mean()\n",
    "    entropy_term = entropy_term.mean()\n",
    "\n",
    "    loss = actor_loss + VF_COEF * value_loss - ENT_COEF * entropy_term\n",
    "    approx_kl = (previous_logprobs - current_logprobs).mean() if global_step > NB_ENVS * NB_STEPS else 0.0\n",
    "\n",
    "    previous_logprobs = current_logprobs.detach().clone()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
    "    optimizer.step()\n",
    "\n",
    "    # logging for the losses + learning rate\n",
    "    if RUN_NAME != None:\n",
    "        writer.add_scalar(\"losses/total_loss\", loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/actor_loss\", actor_loss.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", value_loss.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_term.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl, global_step)\n",
    "        writer.add_scalar(\"charts/learning_rate\", lrnow, global_step)\n",
    "    \n",
    "    \n",
    "# Save the model at the end of training\n",
    "save_path = f\"trained_models/a2c/a2c_config{CONFIG_NUMBER}.pt\"\n",
    "os.makedirs(\"trained_models/a2c\", exist_ok=True)\n",
    "torch.save({\n",
    "    \"model_state_dict\": agent.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"config\": {\n",
    "        \"OBSERVATION_SIZE\": OBSERVATION_SIZE,\n",
    "        \"NB_FRAMES\": NB_FRAMES,\n",
    "        \"NB_ENVS\": NB_ENVS,\n",
    "        \"SEED\": SEED,\n",
    "        \"MAX_EPISODE_LENGTH\": MAX_EPISODE_LENGTH,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"NB_STEPS\": NB_STEPS,\n",
    "        \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,\n",
    "        \"LR_SCHEDULING\": LR_SCHEDULING,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"VF_COEF\": VF_COEF,\n",
    "        \"GAE_LAMBDA\": GAE_LAMBDA,\n",
    "    }\n",
    "}, save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "if RUN_NAME != None:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_racing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
