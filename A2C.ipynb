{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72d2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62bcd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "CONFIG_NUMBER = 11\n",
    "\n",
    "OBSERVATION_SIZE = 64\n",
    "NB_FRAMES = 4\n",
    "NB_ENVS = 12\n",
    "CUDA = False\n",
    "SEED = 2307\n",
    "\n",
    "MAX_EPISODE_LENGTH = 12000\n",
    "LEARNING_RATE = 1e-4\n",
    "NB_STEPS = 2048\n",
    "TOTAL_TIMESTEPS = 2e6\n",
    "RUN_NAME = \"Eleventh Config A2C (LSTM 3-256)\"\n",
    "LR_SCHEDULING = \"Linear\"\n",
    "GAMMA = 0.99\n",
    "VF_COEF = 0.5\n",
    "ENT_COEF = 0.1\n",
    "MAX_GRAD_NORM = 0.5\n",
    "GAE_LAMBDA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73a7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def make_env():\n",
    "    def thunk():\n",
    "        env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.95, domain_randomize=False, continuous=True, max_episode_steps=MAX_EPISODE_LENGTH)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (OBSERVATION_SIZE, OBSERVATION_SIZE))\n",
    "        env = gym.wrappers.GrayscaleObservation(env)\n",
    "        env = gym.wrappers.FrameStackObservation(env, NB_FRAMES)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv([make_env() for _ in range(NB_ENVS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9867dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from TP5\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, nb_frames, image_size):\n",
    "        super(Agent, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.nb_frames = nb_frames\n",
    "\n",
    "        # Actor network\n",
    "       # Actor: CNN → LSTM → Linear\n",
    "        self.actor_cnn, self.actor_lstm, _ = self.build_network(use_lstm=True)\n",
    "        self.actor_linear = layer_init(nn.Linear(256, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))\n",
    "\n",
    "        # Critic: CNN → Linear\n",
    "        self.critic_cnn, _, self.critic_linear = self.build_network(use_lstm=False)\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "\n",
    "\n",
    "    def build_network(self, use_lstm=False):\n",
    "        stride = [4, 2, 1]\n",
    "        kernel_size = [8, 4, 3]\n",
    "        input_channels = [self.nb_frames, 32, 64]\n",
    "        output_channels = [32, 64, 64]\n",
    "        image_size = self.image_size\n",
    "\n",
    "        cnn_layers = []\n",
    "        for i in range(len(stride)):\n",
    "            cnn_layers.append(layer_init(\n",
    "                nn.Conv2d(input_channels[i], output_channels[i], kernel_size[i], stride=stride[i])\n",
    "            ))\n",
    "            cnn_layers.append(nn.Tanh())\n",
    "            image_size = math.floor(((image_size - kernel_size[i]) / stride[i]) + 1)\n",
    "\n",
    "        cnn_layers.append(nn.Flatten())\n",
    "        cnn = nn.Sequential(*cnn_layers)\n",
    "\n",
    "        # Linear input size = 64 * image_size^2\n",
    "        linear_input_size = output_channels[-1] * image_size * image_size\n",
    "\n",
    "        if use_lstm:\n",
    "            lstm = nn.LSTM(input_size=linear_input_size, hidden_size=256, batch_first=True, num_layers=3)\n",
    "            linear = None  # handled after LSTM\n",
    "        else:\n",
    "            lstm = None\n",
    "            linear = nn.Sequential(\n",
    "                layer_init(nn.Linear(linear_input_size, 512)),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        return cnn, lstm, linear\n",
    "\n",
    "\n",
    "    def get_value(self, x):\n",
    "        hidden = self.critic_network(x / 255.0)\n",
    "        return self.critic(hidden)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        x = x / 255.0\n",
    "\n",
    "        # Actor pipeline: CNN → LSTM → Linear\n",
    "        cnn_out = self.actor_cnn(x)               # (B, flat_dim)\n",
    "        lstm_in = cnn_out.unsqueeze(1)            # (B, 1, flat_dim)\n",
    "        lstm_out, _ = self.actor_lstm(lstm_in)    # (B, 1, 512)\n",
    "        actor_hidden = lstm_out.squeeze(1)        # (B, 512)\n",
    "        action_mean = self.actor_linear(actor_hidden)\n",
    "\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        # Critic pipeline: CNN → Linear → Value\n",
    "        critic_hidden = self.critic_linear(self.critic_cnn(x))\n",
    "        value = self.critic(critic_hidden)\n",
    "\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468837cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjimmy-walraff02\u001b[0m (\u001b[33mTFE-proteomics\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (1.6s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\wandb\\run-20250503_094451-pridfhbi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/pridfhbi' target=\"_blank\">Eleventh Config A2C (LSTM 3-256)</a></strong> to <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/pridfhbi' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project/runs/pridfhbi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and CUDA else \"cpu\")\n",
    "agent = Agent(envs, NB_FRAMES, OBSERVATION_SIZE).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "obs = torch.zeros((NB_STEPS, NB_ENVS) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((NB_STEPS, NB_ENVS) + envs.single_action_space.shape).to(device)\n",
    "rewards = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "values = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "current_logprobs = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "previous_logprobs = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "\n",
    "global_step = 0\n",
    "next_obs, _ = envs.reset(seed=SEED)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(NB_ENVS).to(device)\n",
    "\n",
    "if RUN_NAME is not None:\n",
    "    import wandb\n",
    "\n",
    "    config = {\n",
    "        \"OBSERVATION_SIZE\": OBSERVATION_SIZE,\n",
    "        \"NB_FRAMES\": NB_FRAMES,\n",
    "        \"NB_ENVS\": NB_ENVS,\n",
    "        \"SEED\": SEED,\n",
    "        \"MAX_EPISODE_LENGTH\": MAX_EPISODE_LENGTH,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"NB_STEPS\": NB_STEPS,\n",
    "        \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,\n",
    "        \"LR_SCHEDULING\": LR_SCHEDULING,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"VF_COEF\": VF_COEF,\n",
    "        \"CUDA\": CUDA,\n",
    "        \"ENT_COEF\": ENT_COEF,\n",
    "        \"CONFIG_NUMBER\": CONFIG_NUMBER,\n",
    "        \"MAX_GRAD_NORM\": MAX_GRAD_NORM,\n",
    "        \"GAE_LAMBDA\": GAE_LAMBDA,\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        entity=\"Rl2025-project\",\n",
    "        project=\"RL Project\",\n",
    "        name=RUN_NAME,\n",
    "        config=config,\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "        monitor_gym=True,       # auto-upload des vidéos de l'agent\n",
    "        # save_code=True,       # optionnel\n",
    "    )\n",
    "    \n",
    "    writer = SummaryWriter(f\"runs/{RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadde90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=82044, episodic_return=-725.5354838710409\n",
      "global_step=83424, episodic_return=-679.157971014553\n",
      "global_step=86976, episodic_return=-738.6072847682868\n",
      "global_step=87276, episodic_return=-750.2769230770022\n",
      "global_step=108768, episodic_return=-931.0311827958186\n",
      "global_step=126324, episodic_return=-1082.1774647888867\n",
      "global_step=144000, episodic_return=-1137.6770538244223\n",
      "global_step=144000, episodic_return=-1116.9675090253497\n",
      "global_step=144000, episodic_return=-1136.102236421787\n",
      "global_step=144000, episodic_return=-1128.571428571497\n",
      "global_step=144000, episodic_return=-1116.3346613546607\n",
      "global_step=144000, episodic_return=-1123.8754325260236\n",
      "global_step=159528, episodic_return=-584.9460750853628\n",
      "global_step=187716, episodic_return=-868.772789115752\n",
      "global_step=226056, episodic_return=-1120.4545454546217\n",
      "global_step=227436, episodic_return=-1139.209726443828\n",
      "global_step=234432, episodic_return=-720.7835125448729\n",
      "global_step=243096, episodic_return=-844.2440677967095\n",
      "global_step=252780, episodic_return=-1115.7509157509955\n",
      "global_step=255240, episodic_return=-799.2181818182703\n",
      "global_step=270336, episodic_return=-1130.3797468355094\n",
      "global_step=280968, episodic_return=-695.0818181818793\n",
      "global_step=288012, episodic_return=-1133.1210191083444\n",
      "global_step=288012, episodic_return=-1117.5627240144154\n",
      "global_step=288012, episodic_return=-1126.1744966443657\n",
      "global_step=288012, episodic_return=-1149.1525423729322\n",
      "global_step=361620, episodic_return=-1100.097350993536\n",
      "global_step=370068, episodic_return=-1115.708812260616\n",
      "global_step=371448, episodic_return=-1117.2185430464351\n",
      "global_step=371604, episodic_return=-636.736134453831\n",
      "global_step=386232, episodic_return=-916.6825396826563\n",
      "global_step=387108, episodic_return=-1131.8885448917056\n",
      "global_step=396792, episodic_return=-1125.9259259259966\n",
      "global_step=399252, episodic_return=-1134.64052287588\n",
      "global_step=411348, episodic_return=-1202.486206896644\n",
      "global_step=432024, episodic_return=-1129.8245614035764\n",
      "global_step=432024, episodic_return=-1123.3716475096526\n",
      "global_step=432024, episodic_return=-1135.8974358974979\n",
      "global_step=476412, episodic_return=-917.3725490197243\n",
      "global_step=491076, episodic_return=-881.8118143461007\n",
      "global_step=505632, episodic_return=-1133.1103678930408\n",
      "global_step=515460, episodic_return=-1129.2604501608396\n",
      "global_step=515616, episodic_return=-1127.0833333334033\n",
      "global_step=518436, episodic_return=-918.7734265735434\n",
      "global_step=530244, episodic_return=-1128.0821917808908\n",
      "global_step=540804, episodic_return=-1131.818181818247\n",
      "global_step=541596, episodic_return=-924.952117263965\n",
      "global_step=543264, episodic_return=-1133.7349397590997\n",
      "global_step=564444, episodic_return=-1142.8973154363885\n",
      "global_step=576036, episodic_return=-1133.79790940773\n",
      "global_step=612264, episodic_return=-905.7789173790381\n",
      "global_step=617364, episodic_return=-1067.7405405406944\n",
      "global_step=618096, episodic_return=-890.164935065048\n",
      "global_step=620424, episodic_return=-1127.083333333403\n",
      "global_step=627420, episodic_return=-925.9687500001211\n",
      "global_step=651096, episodic_return=-925.1919860628412\n",
      "global_step=659628, episodic_return=-1133.5443037975322\n",
      "global_step=661500, episodic_return=-1123.9135888503379\n",
      "global_step=684816, episodic_return=-1124.9249249249963\n",
      "global_step=687120, episodic_return=-1235.904075235172\n",
      "global_step=701064, episodic_return=-706.9096345515601\n",
      "global_step=708456, episodic_return=-1129.6636085627579\n",
      "global_step=719628, episodic_return=-720.4727272727953\n",
      "global_step=720048, episodic_return=-1136.2416107383167\n",
      "global_step=721032, episodic_return=-838.614937759434\n",
      "global_step=754896, episodic_return=-783.0295774648804\n",
      "global_step=761376, episodic_return=-1129.5302013423493\n",
      "global_step=762108, episodic_return=-1133.5260115607568\n",
      "global_step=780024, episodic_return=-1102.2821917809802\n",
      "global_step=803640, episodic_return=-1128.0821917808908\n",
      "global_step=807888, episodic_return=-927.3040752352287\n",
      "global_step=828828, episodic_return=-1118.8311688312451\n",
      "global_step=831132, episodic_return=-1130.463576159007\n",
      "global_step=852468, episodic_return=-1124.5901639344975\n",
      "global_step=863640, episodic_return=-1128.8256227758689\n",
      "global_step=864060, episodic_return=-1131.6546762590585\n",
      "global_step=865044, episodic_return=-1112.5475285171926\n",
      "global_step=883788, episodic_return=-1050.2797468355895\n",
      "global_step=898908, episodic_return=-1134.64052287588\n",
      "global_step=906120, episodic_return=-1121.708185053455\n",
      "global_step=916296, episodic_return=-1170.253420195561\n",
      "global_step=946512, episodic_return=-617.6086614173668\n",
      "global_step=947652, episodic_return=-1134.3283582090182\n",
      "global_step=951900, episodic_return=-1118.7279151944233\n",
      "global_step=956712, episodic_return=-768.4619047619881\n",
      "global_step=966540, episodic_return=-658.3138576779569\n",
      "global_step=972840, episodic_return=-1132.203389830573\n",
      "global_step=975144, episodic_return=-1133.1210191083442\n",
      "global_step=976512, episodic_return=-1039.6371841156679\n",
      "global_step=983904, episodic_return=-1025.913043478402\n",
      "global_step=1022196, episodic_return=-879.3099667775143\n",
      "global_step=1039356, episodic_return=-775.4754098361485\n",
      "global_step=1042920, episodic_return=-1142.363112391987\n",
      "global_step=1050132, episodic_return=-1118.2156133829772\n",
      "global_step=1057728, episodic_return=-638.4421052632068\n",
      "global_step=1080972, episodic_return=-933.158064516249\n",
      "global_step=1090524, episodic_return=-1128.8135593221018\n",
      "global_step=1095912, episodic_return=-1120.454545454621\n",
      "global_step=1100724, episodic_return=-1130.1587301587963\n",
      "global_step=1110552, episodic_return=-1136.1702127660185\n",
      "global_step=1119156, episodic_return=-1132.2580645161943\n",
      "global_step=1127856, episodic_return=-876.2731543625277\n",
      "global_step=1127916, episodic_return=-1121.348314606817\n",
      "global_step=1146552, episodic_return=-696.6878980892344\n",
      "global_step=1176228, episodic_return=-764.9826366560305\n",
      "global_step=1183368, episodic_return=-1129.8245614035764\n",
      "global_step=1186932, episodic_return=-1125.9259259259964\n",
      "global_step=1194144, episodic_return=-1124.1877256318407\n",
      "global_step=1234536, episodic_return=-1125.5319148936887\n",
      "global_step=1239924, episodic_return=-1134.5794392523992\n",
      "global_step=1244736, episodic_return=-1121.3483146068177\n",
      "global_step=1254564, episodic_return=-1122.8187919463821\n",
      "global_step=1263168, episodic_return=-1126.7399267399971\n",
      "global_step=1271868, episodic_return=-1122.5092250923249\n",
      "global_step=1271928, episodic_return=-1123.0769230769965\n",
      "global_step=1281984, episodic_return=-820.7256227758954\n",
      "global_step=1290564, episodic_return=-1140.6528189911558\n",
      "global_step=1320240, episodic_return=-1119.8606271777762\n",
      "global_step=1327380, episodic_return=-1125.675675675746\n",
      "global_step=1338156, episodic_return=-1126.8292682927527\n",
      "global_step=1367940, episodic_return=-748.7241134752563\n",
      "global_step=1378548, episodic_return=-1112.0000000000834\n",
      "global_step=1383936, episodic_return=-1133.5548172758113\n",
      "global_step=1388748, episodic_return=-1128.3387622150522\n",
      "global_step=1396956, episodic_return=-1224.0947040499166\n",
      "global_step=1407180, episodic_return=-1133.962264151007\n",
      "global_step=1415880, episodic_return=-1132.9268292683569\n",
      "global_step=1415940, episodic_return=-1127.0833333334028\n",
      "global_step=1428840, episodic_return=-873.0978339351246\n",
      "global_step=1434576, episodic_return=-1124.9146757679896\n",
      "global_step=1464252, episodic_return=-1116.3763066202878\n",
      "global_step=1482168, episodic_return=-1128.0821917808905\n",
      "global_step=1511952, episodic_return=-1113.8576779027032\n",
      "global_step=1517964, episodic_return=-943.9486166009135\n",
      "global_step=1522560, episodic_return=-1108.3665338646288\n",
      "global_step=1523280, episodic_return=-1082.5699300700844\n",
      "global_step=1527132, episodic_return=-1215.9070175439394\n",
      "global_step=1532760, episodic_return=-1131.6546762590592\n",
      "global_step=1538844, episodic_return=-876.392307692418\n",
      "global_step=1559892, episodic_return=-1137.293729372998\n",
      "global_step=1559952, episodic_return=-1126.4214046823447\n",
      "global_step=1565892, episodic_return=-860.5380191694397\n",
      "global_step=1572852, episodic_return=-1130.0000000000668\n",
      "global_step=1588836, episodic_return=-624.459044368648\n",
      "global_step=1611444, episodic_return=-744.9710037175508\n",
      "global_step=1618428, episodic_return=-773.3081504703097\n",
      "global_step=1626132, episodic_return=-1235.4756097561594\n",
      "global_step=1664244, episodic_return=-917.2527607363132\n",
      "global_step=1667292, episodic_return=-1115.4411764706679\n",
      "global_step=1671144, episodic_return=-1127.0072992701425\n",
      "global_step=1676772, episodic_return=-1128.5714285714973\n",
      "global_step=1682856, episodic_return=-1127.6315789474377\n",
      "global_step=1703904, episodic_return=-1224.3146757679904\n",
      "global_step=1706808, episodic_return=-695.6880398671752\n",
      "global_step=1708800, episodic_return=-1226.183870967813\n",
      "global_step=1716864, episodic_return=-1131.888544891706\n",
      "global_step=1727808, episodic_return=-991.5338983052212\n",
      "global_step=1732848, episodic_return=-1114.9797570851022\n",
      "global_step=1762440, episodic_return=-1121.9330855019332\n",
      "global_step=1771044, episodic_return=-868.10370370381\n",
      "global_step=1773768, episodic_return=-526.8476190476443\n",
      "global_step=1793628, episodic_return=-680.2137546468972\n",
      "global_step=1798044, episodic_return=-770.5833333334162\n",
      "global_step=1808256, episodic_return=-1123.0769230769968\n",
      "global_step=1815156, episodic_return=-1130.6930693069964\n",
      "global_step=1820784, episodic_return=-1127.1523178808645\n",
      "global_step=1821792, episodic_return=-818.4838709678368\n",
      "global_step=1826868, episodic_return=-1130.1470588235966\n",
      "global_step=1860876, episodic_return=-1128.0575539569038\n",
      "global_step=1867728, episodic_return=-908.3594202899695\n",
      "global_step=1876860, episodic_return=-1134.256055363386\n",
      "global_step=1884180, episodic_return=-946.56666666679\n",
      "global_step=1898136, episodic_return=-849.0269961978198\n",
      "global_step=1900836, episodic_return=-750.0878787879554\n",
      "global_step=1915056, episodic_return=-1116.9435215947622\n",
      "global_step=1942056, episodic_return=-1122.5352112676799\n",
      "global_step=1959168, episodic_return=-1124.187725631841\n",
      "global_step=1964796, episodic_return=-1129.3286219081956\n",
      "global_step=1965804, episodic_return=-1130.158730158797\n",
      "global_step=1970880, episodic_return=-1133.7539432177289\n",
      "global_step=1974876, episodic_return=-671.9241134752384\n",
      "global_step=1982496, episodic_return=-850.1344827587221\n",
      "global_step=2004888, episodic_return=-1126.2820512821208\n",
      "Model saved to trained_models/a2c/a2c_config11.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>charts/episodic_length</td><td>▂▂▂█▁▄██▅█████▄█▆▂█▂▄▃██████▃█▅█▁██▅▄███</td></tr><tr><td>charts/episodic_return</td><td>▆▆▄▂▅▂▁▄▂▄▄▂▂▂▆▂▂▂▇▂▂▆▂▂▂▂▂▂▂▂▂▂▅▆▂▁▂█▆▂</td></tr><tr><td>charts/learning_rate</td><td>██▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁</td></tr><tr><td>global_step</td><td>▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>losses/actor_loss</td><td>▃▂▁▂█▃▄▃▃▃▇▃▃▃▃▂▃▃▃▃▃▃▃▃▃▂▃█▃▃▃▃▂▃▃▃▃▃▃▃</td></tr><tr><td>losses/approx_kl</td><td>▅▄█▇▄▅▅▅▅▄▆▅▆▅▄▅▅▅▃▅▄▅█▄▅▆▄▅▅▁▄▅▃▄▆▆▄▅▄▅</td></tr><tr><td>losses/entropy</td><td>▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>losses/total_loss</td><td>▅▆▄▁▂▄▃▃▃▃█▃▅▃▃▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▃▃▃▃▃▃▃</td></tr><tr><td>losses/value_loss</td><td>██▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>charts/episodic_length</td><td>12000</td></tr><tr><td>charts/episodic_return</td><td>-1126.2821</td></tr><tr><td>charts/learning_rate</td><td>0.0</td></tr><tr><td>global_step</td><td>2004888</td></tr><tr><td>losses/actor_loss</td><td>0.07486</td></tr><tr><td>losses/approx_kl</td><td>-0.00389</td></tr><tr><td>losses/entropy</td><td>4.27304</td></tr><tr><td>losses/total_loss</td><td>-0.12563</td></tr><tr><td>losses/value_loss</td><td>0.45363</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Eleventh Config A2C (LSTM 3-256)</strong> at: <a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/pridfhbi' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project/runs/pridfhbi</a><br> View project at: <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250503_094451-pridfhbi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "while global_step < TOTAL_TIMESTEPS:\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if LR_SCHEDULING == \"Linear\":\n",
    "        frac = 1.0 - global_step / TOTAL_TIMESTEPS\n",
    "        lrnow = frac * LEARNING_RATE\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, NB_STEPS):\n",
    "        global_step += NB_ENVS\n",
    "        obs[step] = next_obs\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "            current_logprobs[step] = logprob\n",
    "\n",
    "        actions[step] = action\n",
    "\n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "        \n",
    "        if \"episode\" in infos:\n",
    "            completed_episodes = infos[\"_episode\"]\n",
    "            episodic_returns = infos[\"episode\"][\"r\"][completed_episodes]\n",
    "            episodic_lengths = infos[\"episode\"][\"l\"][completed_episodes]\n",
    "\n",
    "            for episodic_return, episodic_length in zip(episodic_returns, episodic_lengths):\n",
    "                print(f\"global_step={global_step}, episodic_return={episodic_return}\")\n",
    "                if RUN_NAME != None:\n",
    "                    writer.add_scalar(\"charts/episodic_return\", episodic_return, global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", episodic_length, global_step)\n",
    "        \n",
    "        # Break when one of the environement as reached a terminal state\n",
    "        if torch.any(next_done):\n",
    "            break\n",
    "    \n",
    "    # R = torch.Tensor([0 if next_done[i] == True else values[-1][i] for i in range(len(next_done))]).to(device)\n",
    "    # returns = torch.zeros_like(rewards)\n",
    "    # advantages = torch.zeros_like(rewards)\n",
    "\n",
    "    # for i in reversed(range(step)):\n",
    "    #     R = rewards[i] + GAMMA * R\n",
    "    #     returns[i] = R\n",
    "    #     advantages[i] = returns[i] - values[i]\n",
    "\n",
    "    next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    lastgaelam = 0\n",
    "\n",
    "    for t in reversed(range(step)):\n",
    "        if t == step - 1:\n",
    "            nextnonterminal = 1.0 - next_done\n",
    "            nextvalue = next_value\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - torch.zeros_like(next_done)\n",
    "            nextvalue = values[t + 1]\n",
    "        \n",
    "        delta = rewards[t] + GAMMA * nextvalue * nextnonterminal - values[t]\n",
    "        advantages[t] = lastgaelam = delta + GAMMA * GAE_LAMBDA * nextnonterminal * lastgaelam\n",
    "\n",
    "    returns = advantages + values\n",
    "\n",
    "    # Normalize the advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    value_loss = torch.zeros((step, NB_ENVS))\n",
    "    actor_loss = torch.zeros((step, NB_ENVS))\n",
    "    entropy_term = torch.zeros((step, NB_ENVS))\n",
    "\n",
    "    for i in range(step):\n",
    "        _, logprob, ent, value = agent.get_action_and_value(obs[i], actions[i])\n",
    "        value = value.flatten()\n",
    "\n",
    "        actor_loss[i] = -logprob * advantages[i]\n",
    "        value_loss[i] = (returns[i] - value)**2\n",
    "        entropy_term[i] = ent\n",
    "\n",
    "    actor_loss = actor_loss.mean()\n",
    "    value_loss = 0.5 * value_loss.mean()\n",
    "    entropy_term = entropy_term.mean()\n",
    "\n",
    "    loss = actor_loss + VF_COEF * value_loss - ENT_COEF * entropy_term\n",
    "    approx_kl = (previous_logprobs - current_logprobs).mean() if global_step > NB_ENVS * NB_STEPS else 0.0\n",
    "\n",
    "    previous_logprobs = current_logprobs.detach().clone()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
    "    optimizer.step()\n",
    "\n",
    "    # logging for the losses + learning rate\n",
    "    if RUN_NAME != None:\n",
    "        writer.add_scalar(\"losses/total_loss\", loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/actor_loss\", actor_loss.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", value_loss.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_term.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl, global_step)\n",
    "        writer.add_scalar(\"charts/learning_rate\", lrnow, global_step)\n",
    "    \n",
    "    \n",
    "# Save the model at the end of training\n",
    "save_path = f\"trained_models/a2c/a2c_config{CONFIG_NUMBER}.pt\"\n",
    "os.makedirs(\"trained_models/a2c\", exist_ok=True)\n",
    "torch.save({\n",
    "    \"model_state_dict\": agent.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"config\": {\n",
    "        \"OBSERVATION_SIZE\": OBSERVATION_SIZE,\n",
    "        \"NB_FRAMES\": NB_FRAMES,\n",
    "        \"NB_ENVS\": NB_ENVS,\n",
    "        \"SEED\": SEED,\n",
    "        \"MAX_EPISODE_LENGTH\": MAX_EPISODE_LENGTH,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"NB_STEPS\": NB_STEPS,\n",
    "        \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,\n",
    "        \"LR_SCHEDULING\": LR_SCHEDULING,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"VF_COEF\": VF_COEF,\n",
    "        \"MAX_GRAD_NORM\": MAX_GRAD_NORM,\n",
    "        \"ENT_COEF\": ENT_COEF,\n",
    "        \"GAE_LAMBDA\": GAE_LAMBDA,\n",
    "    }\n",
    "}, save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "if RUN_NAME != None:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlprojet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
