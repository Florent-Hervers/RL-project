{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72d2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62bcd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "CONFIG_NUMBER = 1\n",
    "\n",
    "OBSERVATION_SIZE = 84\n",
    "NB_FRAMES = 4\n",
    "NB_ENVS = 4\n",
    "CUDA = False\n",
    "SEED = 2307\n",
    "\n",
    "MAX_EPISODE_LENGTH = 12000\n",
    "LEARNING_RATE = 1e-4\n",
    "NB_STEPS = 256\n",
    "TOTAL_TIMESTEPS = 2e6\n",
    "RUN_NAME = \"FirstConfigA2C\"\n",
    "LR_SCHEDULING = \"Linear\"\n",
    "GAMMA = 0.99\n",
    "VF_COEF = 0.5\n",
    "ENT_COEF = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73a7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def make_env():\n",
    "    def thunk():\n",
    "        env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.95, domain_randomize=False, continuous=True, max_episode_steps=MAX_EPISODE_LENGTH)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (OBSERVATION_SIZE, OBSERVATION_SIZE))\n",
    "        env = gym.wrappers.GrayscaleObservation(env)\n",
    "        env = gym.wrappers.FrameStackObservation(env, NB_FRAMES)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv([make_env() for _ in range(NB_ENVS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9867dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from TP5\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, nb_frames, image_size):\n",
    "        super(Agent, self).__init__()\n",
    "        stride = [4, 2, 1]\n",
    "        kernel_size = [8, 4, 3]\n",
    "        input_channels = [nb_frames, 32, 64]\n",
    "        output_channels = [32, 64, 64]\n",
    "\n",
    "        self.output_image_size = math.floor(math.floor(((math.floor(((image_size - 8) / 4) + 1) - 4) / 2) + 1) - 3 + 1)\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(stride)):\n",
    "            layers.append(layer_init(nn.Conv2d(input_channels[i], output_channels[i], kernel_size[i], stride=stride[i])))\n",
    "            layers.append(nn.ReLU())\n",
    "            image_size = math.floor(((image_size - kernel_size[i]) / stride[i]) + 1)\n",
    "        \n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(layer_init(nn.Linear(output_channels[-1] * image_size * image_size, 512)))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "        \n",
    "        self.actor_mean = layer_init(\n",
    "            nn.Linear(512, np.prod(envs.single_action_space.shape)),\n",
    "            std=0.01\n",
    "        )\n",
    "\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 255.0)\n",
    "        action_mean = self.actor_mean(hidden)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468837cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflo230702\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Session/Documents/Universite/Master 2/Reinforcement learning/Project/wandb/run-20250424_220046-lrf9oqig</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/lrf9oqig' target=\"_blank\">FirstConfigA2C</a></strong> to <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/lrf9oqig' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project/runs/lrf9oqig</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and CUDA else \"cpu\")\n",
    "agent = Agent(envs, NB_FRAMES, OBSERVATION_SIZE).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "obs = torch.zeros((NB_STEPS, NB_ENVS) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((NB_STEPS, NB_ENVS) + envs.single_action_space.shape).to(device)\n",
    "rewards = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "values = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "\n",
    "global_step = 0\n",
    "next_obs, _ = envs.reset(seed=SEED)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(NB_ENVS).to(device)\n",
    "\n",
    "if RUN_NAME is not None:\n",
    "    import wandb\n",
    "\n",
    "    config = {\n",
    "        \"OBSERVATION_SIZE\": OBSERVATION_SIZE,\n",
    "        \"NB_FRAMES\": NB_FRAMES,\n",
    "        \"NB_ENVS\": NB_ENVS,\n",
    "        \"SEED\": SEED,\n",
    "        \"MAX_EPISODE_LENGTH\": MAX_EPISODE_LENGTH,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"NB_STEPS\": NB_STEPS,\n",
    "        \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,\n",
    "        \"LR_SCHEDULING\": LR_SCHEDULING,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"VF_COEF\": VF_COEF,\n",
    "        \"CUDA\": CUDA,\n",
    "        \"ENT_COEF\": ENT_COEF,\n",
    "        \"CONFIG_NUMBER\": CONFIG_NUMBER\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        entity=\"Rl2025-project\",\n",
    "        project=\"RL Project\",\n",
    "        name=RUN_NAME,\n",
    "        config=config,\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "        monitor_gym=True,       # auto-upload des vidéos de l'agent\n",
    "        # save_code=True,       # optionnel\n",
    "    )\n",
    "    \n",
    "    writer = SummaryWriter(f\"runs/{RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceadde90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=48000, episodic_return=-1115.0141643060326\n",
      "global_step=48000, episodic_return=-1111.971830986\n",
      "global_step=48000, episodic_return=-1098.9169675091248\n",
      "global_step=48000, episodic_return=-1126.5175718850553\n",
      "global_step=85280, episodic_return=-905.2177215190987\n",
      "global_step=86848, episodic_return=-1007.3057324842146\n",
      "global_step=90504, episodic_return=-979.604301075398\n",
      "global_step=92024, episodic_return=-1103.0845637585535\n",
      "global_step=95944, episodic_return=-285.09473684209894\n",
      "global_step=96712, episodic_return=-313.1862068965439\n",
      "global_step=120732, episodic_return=-744.7030651341776\n",
      "global_step=126632, episodic_return=-805.8419580420516\n",
      "global_step=128316, episodic_return=-249.13154362415682\n",
      "global_step=135224, episodic_return=-235.8027522935745\n",
      "global_step=138508, episodic_return=-1073.949579832069\n",
      "global_step=140800, episodic_return=-395.4062499999986\n",
      "global_step=142276, episodic_return=-249.87049180327438\n",
      "global_step=143948, episodic_return=-1141.3680781759583\n",
      "global_step=144876, episodic_return=-213.86111111110876\n",
      "global_step=148840, episodic_return=-213.67878787878615\n",
      "global_step=150112, episodic_return=-167.40886075949402\n",
      "global_step=152232, episodic_return=-156.13024911032073\n",
      "global_step=190280, episodic_return=-1149.4584837545628\n",
      "global_step=191952, episodic_return=-1161.672473867635\n",
      "global_step=198116, episodic_return=-1141.7808219178728\n",
      "global_step=199740, episodic_return=-256.8287671232832\n",
      "global_step=200236, episodic_return=-1065.9420289856537\n",
      "global_step=212832, episodic_return=-324.81235955055286\n",
      "global_step=238284, episodic_return=-1009.2105263159574\n",
      "global_step=246120, episodic_return=-1092.5373134329332\n",
      "global_step=247744, episodic_return=-981.758957654892\n",
      "global_step=253800, episodic_return=-398.5280254777069\n",
      "global_step=259224, episodic_return=-211.66112759643704\n",
      "global_step=260836, episodic_return=-1119.5804195804972\n",
      "global_step=294124, episodic_return=-1114.7540983607385\n",
      "global_step=295748, episodic_return=-1047.1760797343788\n",
      "global_step=307228, episodic_return=-1138.5665529010882\n",
      "global_step=308840, episodic_return=-1106.250000000049\n",
      "global_step=342128, episodic_return=-1150.8771929825052\n",
      "global_step=343752, episodic_return=-1156.3758389262218\n",
      "global_step=355232, episodic_return=-1146.1538461539042\n",
      "global_step=356844, episodic_return=-1159.8662207358254\n",
      "global_step=390132, episodic_return=-1166.2162162162513\n",
      "global_step=391756, episodic_return=-1174.1697416974448\n",
      "global_step=403236, episodic_return=-1153.947368421099\n",
      "global_step=404848, episodic_return=-1152.218430034183\n",
      "global_step=438136, episodic_return=-1102.5270758123672\n",
      "global_step=439760, episodic_return=-1175.609756097587\n",
      "global_step=440480, episodic_return=-101.29834983498345\n",
      "global_step=441104, episodic_return=-96.99509202453983\n",
      "global_step=451240, episodic_return=-1181.6176470588443\n",
      "global_step=452852, episodic_return=-1173.9776951673143\n",
      "global_step=486140, episodic_return=-1166.6666666667013\n",
      "global_step=489108, episodic_return=-1049.6503496505127\n",
      "global_step=499244, episodic_return=-1155.8359621451534\n",
      "global_step=500856, episodic_return=-1142.9657794677355\n",
      "global_step=534144, episodic_return=-980.1857585141147\n",
      "global_step=537112, episodic_return=-923.4848484849908\n",
      "global_step=547248, episodic_return=-953.2051282053195\n",
      "global_step=548860, episodic_return=-1043.9716312058397\n",
      "global_step=559468, episodic_return=-704.1230215828026\n",
      "global_step=585116, episodic_return=-1132.475884244439\n",
      "global_step=595252, episodic_return=-1081.9672131148732\n",
      "global_step=596864, episodic_return=-1011.387900356054\n",
      "global_step=607472, episodic_return=-1161.5384615385008\n",
      "global_step=624640, episodic_return=-790.1313993175002\n",
      "global_step=633120, episodic_return=-1095.1048951049947\n",
      "global_step=644868, episodic_return=-1070.6624605679601\n",
      "global_step=655476, episodic_return=-941.2587412588898\n",
      "global_step=663888, episodic_return=-226.47630662020694\n",
      "global_step=671440, episodic_return=-1079.4774193550095\n",
      "global_step=672312, episodic_return=-744.855223880675\n",
      "global_step=680796, episodic_return=-159.28139534883942\n",
      "global_step=681124, episodic_return=-1031.2500000001542\n",
      "global_step=685056, episodic_return=-440.07491856678706\n",
      "global_step=696556, episodic_return=-543.8310344827969\n",
      "global_step=698176, episodic_return=-367.633333333325\n",
      "global_step=701068, episodic_return=-309.017197452221\n",
      "global_step=701964, episodic_return=-173.29858657243824\n",
      "global_step=702728, episodic_return=-66.3814332247562\n",
      "global_step=703588, episodic_return=-551.5098901099283\n",
      "global_step=709468, episodic_return=-222.7846153846127\n",
      "global_step=709816, episodic_return=-167.8376623376628\n",
      "global_step=722672, episodic_return=-491.14447949528505\n",
      "global_step=734596, episodic_return=-466.35125448029993\n",
      "global_step=741080, episodic_return=-881.9391459076056\n",
      "global_step=747264, episodic_return=-198.84444444444352\n",
      "global_step=747644, episodic_return=-172.15384615384704\n",
      "global_step=749688, episodic_return=-113.73783783783792\n",
      "global_step=757820, episodic_return=-836.3636363637978\n",
      "global_step=758664, episodic_return=-308.71304347825367\n",
      "global_step=763680, episodic_return=-320.7864406779559\n",
      "global_step=770676, episodic_return=-765.0793650795182\n",
      "global_step=779744, episodic_return=-244.30821917807748\n",
      "global_step=789136, episodic_return=-418.85682656826884\n",
      "global_step=804604, episodic_return=-1176.9342465754544\n",
      "global_step=805544, episodic_return=-419.7222222222265\n",
      "global_step=806528, episodic_return=-110.86296296296302\n",
      "global_step=806668, episodic_return=-754.4554455447137\n",
      "global_step=810596, episodic_return=-135.95620437956276\n",
      "global_step=827748, episodic_return=-960.1351351353037\n",
      "global_step=853548, episodic_return=-1082.7361563519053\n",
      "global_step=854532, episodic_return=-1178.851963746247\n",
      "global_step=858600, episodic_return=-1079.0560471977544\n",
      "global_step=875752, episodic_return=-1163.636363636401\n",
      "global_step=901552, episodic_return=-1003.5087719300018\n",
      "global_step=902536, episodic_return=-1054.6712802769596\n",
      "global_step=906604, episodic_return=-1053.6585365855212\n",
      "global_step=923756, episodic_return=-1028.3582089553904\n",
      "global_step=929072, episodic_return=-572.3910891089471\n",
      "global_step=934820, episodic_return=-842.0960264901707\n",
      "global_step=942352, episodic_return=-857.4641509435389\n",
      "global_step=971760, episodic_return=-996.407185628907\n",
      "global_step=977076, episodic_return=-1005.8608058609714\n",
      "global_step=982824, episodic_return=-1125.925925926006\n",
      "global_step=990356, episodic_return=-1049.6503496505104\n",
      "global_step=992416, episodic_return=-272.03243243242747\n",
      "global_step=1019764, episodic_return=-940.3508771931758\n",
      "global_step=1025080, episodic_return=-1018.1818181819878\n",
      "global_step=1037044, episodic_return=-472.2388714733689\n",
      "global_step=1038360, episodic_return=-976.5042979944435\n",
      "global_step=1040420, episodic_return=-1000.0000000001894\n",
      "global_step=1041120, episodic_return=-147.89407665505263\n",
      "global_step=1046924, episodic_return=-226.31958041956935\n",
      "global_step=1054740, episodic_return=-261.1863945578182\n",
      "global_step=1075028, episodic_return=-490.89513108616256\n",
      "global_step=1085048, episodic_return=-998.7421383649346\n",
      "global_step=1088424, episodic_return=-1098.3050847458717\n",
      "global_step=1089124, episodic_return=-997.7941176472146\n",
      "global_step=1103404, episodic_return=-381.26043165466876\n",
      "global_step=1123032, episodic_return=-935.294117647257\n",
      "global_step=1130792, episodic_return=-1041.5503355706132\n",
      "global_step=1133052, episodic_return=-1140.0749063671\n",
      "global_step=1137236, episodic_return=-339.51538461537655\n",
      "global_step=1143924, episodic_return=-243.4705882352902\n",
      "global_step=1151408, episodic_return=-1016.6089965399611\n",
      "global_step=1164520, episodic_return=-815.8522336770899\n",
      "global_step=1181056, episodic_return=-1011.0749185669599\n",
      "global_step=1191928, episodic_return=-999.401197604984\n",
      "global_step=1192100, episodic_return=-646.9642335766948\n",
      "global_step=1197564, episodic_return=-180.3825396825398\n",
      "global_step=1199412, episodic_return=-807.142857143032\n",
      "global_step=1200896, episodic_return=-267.0328767123225\n",
      "global_step=1211360, episodic_return=-325.3292682926714\n",
      "global_step=1214688, episodic_return=-429.80612244898003\n",
      "global_step=1227108, episodic_return=-667.7213592233612\n",
      "global_step=1232608, episodic_return=-213.839589442813\n",
      "global_step=1235096, episodic_return=-544.4827586207249\n",
      "global_step=1245568, episodic_return=-750.4885993486156\n",
      "global_step=1259364, episodic_return=-859.0257879657678\n",
      "global_step=1271372, episodic_return=-220.78853046594583\n",
      "global_step=1280612, episodic_return=-948.0620155040601\n",
      "global_step=1283100, episodic_return=-669.8795180724276\n",
      "global_step=1293572, episodic_return=-913.3105802049591\n",
      "global_step=1319376, episodic_return=-916.7235494882422\n",
      "global_step=1328616, episodic_return=-851.7571884985538\n",
      "global_step=1331104, episodic_return=-1018.4713375797893\n",
      "global_step=1341576, episodic_return=-984.7222222223897\n",
      "global_step=1345460, episodic_return=-440.36308724833054\n",
      "global_step=1367380, episodic_return=-969.2307692309206\n",
      "global_step=1368396, episodic_return=-719.624324324397\n",
      "global_step=1379108, episodic_return=-1166.7774086379084\n",
      "global_step=1384764, episodic_return=-955.7766233767552\n",
      "global_step=1393640, episodic_return=-249.14479495267702\n",
      "global_step=1395348, episodic_return=-307.2184397163056\n",
      "global_step=1415384, episodic_return=-703.5714285715472\n",
      "global_step=1416400, episodic_return=-1014.9466192172509\n",
      "global_step=1441644, episodic_return=-1126.5175718850542\n",
      "global_step=1443352, episodic_return=-930.6273062732386\n",
      "global_step=1446180, episodic_return=-794.7950495050392\n",
      "global_step=1452424, episodic_return=-146.04444444444405\n",
      "global_step=1463388, episodic_return=-895.3846153847937\n",
      "global_step=1467716, episodic_return=-299.3043010752625\n",
      "global_step=1471420, episodic_return=-692.10952380959\n",
      "global_step=1489648, episodic_return=-1012.7490039842244\n",
      "global_step=1494936, episodic_return=-657.1115942029571\n",
      "global_step=1495140, episodic_return=-845.0563106797127\n",
      "global_step=1519424, episodic_return=-945.4545454546831\n",
      "global_step=1537652, episodic_return=-1023.9700374533478\n",
      "global_step=1537832, episodic_return=-924.9125000001201\n",
      "global_step=1542940, episodic_return=-891.8032786886965\n",
      "global_step=1548308, episodic_return=-265.8095890410916\n",
      "global_step=1567428, episodic_return=-1087.0431893688728\n",
      "global_step=1585656, episodic_return=-1009.8591549297449\n",
      "global_step=1590944, episodic_return=-1166.7774086379084\n",
      "global_step=1596312, episodic_return=-771.9063545152543\n",
      "global_step=1615432, episodic_return=-1138.311688311749\n",
      "global_step=1633660, episodic_return=-829.8701298703022\n",
      "global_step=1638948, episodic_return=-654.1984732825645\n",
      "global_step=1642312, episodic_return=-128.60000000000133\n",
      "global_step=1644316, episodic_return=-1002.7210884355584\n",
      "global_step=1663436, episodic_return=-805.1779935276846\n",
      "global_step=1686952, episodic_return=-998.6348122868724\n",
      "global_step=1690316, episodic_return=-861.3418530352947\n",
      "global_step=1691356, episodic_return=-547.8000000000222\n",
      "global_step=1692320, episodic_return=-832.3024054984232\n",
      "global_step=1697628, episodic_return=-267.3024096385499\n",
      "global_step=1717936, episodic_return=-709.067441860535\n",
      "global_step=1738320, episodic_return=-1049.2647058824955\n",
      "global_step=1740324, episodic_return=-929.4520547946732\n",
      "global_step=1745632, episodic_return=-1032.7137546469937\n",
      "global_step=1747276, episodic_return=-206.98075709779062\n",
      "global_step=1765940, episodic_return=-1123.6947791165403\n",
      "global_step=1788328, episodic_return=-1179.4520547945435\n",
      "global_step=1793636, episodic_return=-639.84962406024\n",
      "global_step=1795280, episodic_return=-999.2700729928905\n",
      "global_step=1803308, episodic_return=-108.47101449275478\n",
      "global_step=1813944, episodic_return=-969.2307692309136\n",
      "global_step=1836332, episodic_return=-864.4067796611874\n",
      "global_step=1841640, episodic_return=-688.8888888890173\n",
      "global_step=1851312, episodic_return=-1001.857585139485\n",
      "global_step=1861948, episodic_return=-790.3225806453163\n",
      "global_step=1884336, episodic_return=-1088.8888888889953\n",
      "global_step=1889644, episodic_return=-1050.0000000001464\n",
      "global_step=1899316, episodic_return=-948.4276729561683\n",
      "global_step=1909952, episodic_return=-1011.2582781458832\n",
      "global_step=1929608, episodic_return=-637.5054054054596\n",
      "global_step=1932340, episodic_return=-848.8294314382604\n",
      "global_step=1937648, episodic_return=-1026.9230769232586\n",
      "global_step=1957956, episodic_return=-942.957746479035\n",
      "global_step=1977612, episodic_return=-804.9822064058661\n",
      "global_step=1980344, episodic_return=-983.8709677420994\n",
      "global_step=1985652, episodic_return=-888.8888888890631\n",
      "global_step=1994880, episodic_return=-426.23684210526795\n",
      "Model saved to trained_models/a2c/a2c_config1.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>charts/episodic_length</td><td>█▂▃▂▁████████▄▁▅█▃███▁██▆▄▃███▅██▆██████</td></tr><tr><td>charts/episodic_return</td><td>▇▇▂▁▂▁▁▂█▅█▄█▂▂▂▅▁▂▄▄▇▅▃▂▁▄▃▇▇▂▁▂▅▄█▄▃▂▂</td></tr><tr><td>charts/learning_rate</td><td>███▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇████</td></tr><tr><td>losses/actor_loss</td><td>▃▃▃▄▂▂▄▃▃▅▄▄▅▃▂▂▄▃▃█▅▃▃▃▄▃▃▃▃▃▇▁▁▂▅▂▃▂▅▂</td></tr><tr><td>losses/entropy</td><td>█▁█▄███▂████████████████████████████▇███</td></tr><tr><td>losses/total_loss</td><td>▁▁▁▂▅▁▂▁▁▁▂▁▁▂▁▂█▃▃▁▂▁▁▁▁▃▁▁▁▂▂▂▁▃▁▁▃▁▁▁</td></tr><tr><td>losses/value_loss</td><td>▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▂▂█▁▅▁▁▇▂▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>charts/episodic_length</td><td>4316</td></tr><tr><td>charts/episodic_return</td><td>-426.23685</td></tr><tr><td>charts/learning_rate</td><td>0.0</td></tr><tr><td>global_step</td><td>2000000</td></tr><tr><td>losses/actor_loss</td><td>246.09448</td></tr><tr><td>losses/entropy</td><td>1088.6373</td></tr><tr><td>losses/total_loss</td><td>925.8069</td></tr><tr><td>losses/value_loss</td><td>1381.1975</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FirstConfigA2C</strong> at: <a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/lrf9oqig' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project/runs/lrf9oqig</a><br> View project at: <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250424_220046-lrf9oqig/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "while global_step < TOTAL_TIMESTEPS:\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if LR_SCHEDULING == \"Linear\":\n",
    "        frac = 1.0 - global_step / TOTAL_TIMESTEPS\n",
    "        lrnow = frac * LEARNING_RATE\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, NB_STEPS):\n",
    "        global_step += NB_ENVS\n",
    "        obs[step] = next_obs\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "\n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "        \n",
    "        if \"episode\" in infos:\n",
    "            completed_episodes = infos[\"_episode\"]\n",
    "            episodic_returns = infos[\"episode\"][\"r\"][completed_episodes]\n",
    "            episodic_lengths = infos[\"episode\"][\"l\"][completed_episodes]\n",
    "\n",
    "            for episodic_return, episodic_length in zip(episodic_returns, episodic_lengths):\n",
    "                print(f\"global_step={global_step}, episodic_return={episodic_return}\")\n",
    "                if RUN_NAME != None:\n",
    "                    writer.add_scalar(\"charts/episodic_return\", episodic_return, global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", episodic_length, global_step)\n",
    "        \n",
    "        # Break when one of the environement as reached a terminal state\n",
    "        if torch.any(next_done):\n",
    "            break\n",
    "    \n",
    "    R = torch.Tensor([0 if next_done[i] == True else values[-1][i] for i in range(len(next_done))]).to(device)\n",
    "    value_loss = 0\n",
    "    actor_loss = 0\n",
    "    entropy_term = 0\n",
    "    # TODO check that what's done here is coherant with the slides of lecture 6 (slide 17 and 21): the implementation is based on the paper of A3c https://arxiv.org/pdf/1602.01783\n",
    "    for i in range(step , 0, -1):\n",
    "        R = rewards[i] + GAMMA * R\n",
    "\n",
    "        _ , logprob, ent, value = agent.get_action_and_value(obs[i], actions[i])\n",
    "        value = value.flatten()\n",
    "        \n",
    "        # Reuse here the non_grad values as we don't want to update the value network with the actor loss\n",
    "        actor_loss += -logprob * (R - values[i])\n",
    "        value_loss += (R - value)**2\n",
    "        entropy_term += ent\n",
    "\n",
    "    # TODO add the computation of the entropy, add it to the loss and log the relevant metrics\n",
    "    loss = (actor_loss + VF_COEF * value_loss - ENT_COEF * entropy_term).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # logging for the losses + learning rate\n",
    "    if RUN_NAME != None:\n",
    "        writer.add_scalar(\"losses/total_loss\", loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/actor_loss\", actor_loss.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", value_loss.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_term.mean().item(), global_step)\n",
    "        writer.add_scalar(\"charts/learning_rate\", lrnow, global_step)\n",
    "    \n",
    "    \n",
    "# Save the model at the end of training\n",
    "save_path = f\"trained_models/a2c/a2c_config{CONFIG_NUMBER}.pt\"\n",
    "os.makedirs(\"trained_models/a2c\", exist_ok=True)\n",
    "torch.save({\n",
    "    \"model_state_dict\": agent.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"config\": {\n",
    "        \"OBSERVATION_SIZE\": OBSERVATION_SIZE,\n",
    "        \"NB_FRAMES\": NB_FRAMES,\n",
    "        \"NB_ENVS\": NB_ENVS,\n",
    "        \"SEED\": SEED,\n",
    "        \"MAX_EPISODE_LENGTH\": MAX_EPISODE_LENGTH,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"NB_STEPS\": NB_STEPS,\n",
    "        \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,\n",
    "        \"LR_SCHEDULING\": LR_SCHEDULING,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"VF_COEF\": VF_COEF\n",
    "    }\n",
    "}, save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "if RUN_NAME != None:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_racing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
