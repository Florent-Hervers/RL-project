{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72d2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62bcd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "CONFIG_NUMBER = 8\n",
    "\n",
    "OBSERVATION_SIZE = 64\n",
    "NB_FRAMES = 4\n",
    "NB_ENVS = 12\n",
    "CUDA = False\n",
    "SEED = 2307\n",
    "\n",
    "MAX_EPISODE_LENGTH = 12000\n",
    "LEARNING_RATE = 1e-4\n",
    "NB_STEPS = 512\n",
    "TOTAL_TIMESTEPS = 2e6\n",
    "RUN_NAME = \"Eighth Config A2C (LSTM)\"\n",
    "LR_SCHEDULING = \"Linear\"\n",
    "GAMMA = 0.99\n",
    "VF_COEF = 0.5\n",
    "ENT_COEF = 0.1\n",
    "MAX_GRAD_NORM = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73a7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def make_env():\n",
    "    def thunk():\n",
    "        env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.95, domain_randomize=False, continuous=True, max_episode_steps=MAX_EPISODE_LENGTH)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (OBSERVATION_SIZE, OBSERVATION_SIZE))\n",
    "        env = gym.wrappers.GrayscaleObservation(env)\n",
    "        env = gym.wrappers.FrameStackObservation(env, NB_FRAMES)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv([make_env() for _ in range(NB_ENVS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from TP5\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, nb_frames, image_size):\n",
    "        super(Agent, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.nb_frames = nb_frames\n",
    "\n",
    "        # Actor network\n",
    "       # Actor: CNN → LSTM → Linear\n",
    "        self.actor_cnn, self.actor_lstm, _ = self.build_network(use_lstm=True)\n",
    "        self.actor_linear = layer_init(nn.Linear(128, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))\n",
    "\n",
    "        # Critic: CNN → Linear\n",
    "        self.critic_cnn, _, self.critic_linear = self.build_network(use_lstm=False)\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "\n",
    "\n",
    "    def build_network(self, use_lstm=False):\n",
    "        stride = [4, 2, 1]\n",
    "        kernel_size = [8, 4, 3]\n",
    "        input_channels = [self.nb_frames, 32, 64]\n",
    "        output_channels = [32, 64, 64]\n",
    "        image_size = self.image_size\n",
    "\n",
    "        cnn_layers = []\n",
    "        for i in range(len(stride)):\n",
    "            cnn_layers.append(layer_init(\n",
    "                nn.Conv2d(input_channels[i], output_channels[i], kernel_size[i], stride=stride[i])\n",
    "            ))\n",
    "            cnn_layers.append(nn.Tanh())\n",
    "            image_size = math.floor(((image_size - kernel_size[i]) / stride[i]) + 1)\n",
    "\n",
    "        cnn_layers.append(nn.Flatten())\n",
    "        cnn = nn.Sequential(*cnn_layers)\n",
    "\n",
    "        # Linear input size = 64 * image_size^2\n",
    "        linear_input_size = output_channels[-1] * image_size * image_size\n",
    "\n",
    "        if use_lstm:\n",
    "            lstm = nn.LSTM(input_size=linear_input_size, hidden_size=128, batch_first=True, num_layers=1)\n",
    "            linear = None  # handled after LSTM\n",
    "        else:\n",
    "            lstm = None\n",
    "            linear = nn.Sequential(\n",
    "                layer_init(nn.Linear(linear_input_size, 512)),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        return cnn, lstm, linear\n",
    "\n",
    "\n",
    "    def get_value(self, x):\n",
    "        hidden = self.critic_network(x / 255.0)\n",
    "        return self.critic(hidden)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        x = x / 255.0\n",
    "\n",
    "        # Actor pipeline: CNN → LSTM → Linear\n",
    "        cnn_out = self.actor_cnn(x)               # (B, flat_dim)\n",
    "        lstm_in = cnn_out.unsqueeze(1)            # (B, 1, flat_dim)\n",
    "        lstm_out, _ = self.actor_lstm(lstm_in)    # (B, 1, 512)\n",
    "        actor_hidden = lstm_out.squeeze(1)        # (B, 512)\n",
    "        action_mean = self.actor_linear(actor_hidden)\n",
    "\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        # Critic pipeline: CNN → Linear → Value\n",
    "        critic_hidden = self.critic_linear(self.critic_cnn(x))\n",
    "        value = self.critic(critic_hidden)\n",
    "\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "468837cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\wandb\\run-20250501_102056-lz0rdgvo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/lz0rdgvo' target=\"_blank\">Eighth Config A2C (LSTM)</a></strong> to <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/lz0rdgvo' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project/runs/lz0rdgvo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and CUDA else \"cpu\")\n",
    "agent = Agent(envs, NB_FRAMES, OBSERVATION_SIZE).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "obs = torch.zeros((NB_STEPS, NB_ENVS) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((NB_STEPS, NB_ENVS) + envs.single_action_space.shape).to(device)\n",
    "rewards = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "values = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "current_logprobs = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "previous_logprobs = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "\n",
    "global_step = 0\n",
    "next_obs, _ = envs.reset(seed=SEED)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(NB_ENVS).to(device)\n",
    "\n",
    "if RUN_NAME is not None:\n",
    "    import wandb\n",
    "\n",
    "    config = {\n",
    "        \"OBSERVATION_SIZE\": OBSERVATION_SIZE,\n",
    "        \"NB_FRAMES\": NB_FRAMES,\n",
    "        \"NB_ENVS\": NB_ENVS,\n",
    "        \"SEED\": SEED,\n",
    "        \"MAX_EPISODE_LENGTH\": MAX_EPISODE_LENGTH,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"NB_STEPS\": NB_STEPS,\n",
    "        \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,\n",
    "        \"LR_SCHEDULING\": LR_SCHEDULING,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"VF_COEF\": VF_COEF,\n",
    "        \"CUDA\": CUDA,\n",
    "        \"ENT_COEF\": ENT_COEF,\n",
    "        \"CONFIG_NUMBER\": CONFIG_NUMBER\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        entity=\"Rl2025-project\",\n",
    "        project=\"RL Project\",\n",
    "        name=RUN_NAME,\n",
    "        config=config,\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "        monitor_gym=True,       # auto-upload des vidéos de l'agent\n",
    "        # save_code=True,       # optionnel\n",
    "    )\n",
    "    \n",
    "    writer = SummaryWriter(f\"runs/{RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceadde90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=81684, episodic_return=-606.5109311741301\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m     current_logprobs[step] \u001b[38;5;241m=\u001b[39m logprob\n\u001b[0;32m     17\u001b[0m actions[step] \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m---> 19\u001b[0m next_obs, reward, terminations, truncations, infos \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m next_done \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(terminations, truncations)\n\u001b[0;32m     21\u001b[0m rewards[step] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(reward)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\vector\\sync_vector_env.py:222\u001b[0m, in \u001b[0;36mSyncVectorEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncations[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     (\n\u001b[0;32m    217\u001b[0m         env_obs,\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[i],\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminations[i],\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncations[i],\n\u001b[0;32m    221\u001b[0m         env_info,\n\u001b[1;32m--> 222\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m observations\u001b[38;5;241m.\u001b[39mappend(env_obs)\n\u001b[0;32m    225\u001b[0m infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_info(infos, env_info, i)\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\wrappers\\stateful_observation.py:416\u001b[0m, in \u001b[0;36mFrameStackObservation.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \n\u001b[0;32m    410\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and info from the environment\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 416\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_queue\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[0;32m    419\u001b[0m     updated_obs \u001b[38;5;241m=\u001b[39m deepcopy(\n\u001b[0;32m    420\u001b[0m         concatenate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_queue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstacked_obs)\n\u001b[0;32m    421\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\core.py:550\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    548\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\core.py:550\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    548\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\wrappers\\common.py:513\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:563\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mStep(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS, \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS\n\u001b[1;32m--> 563\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_pixels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m step_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    566\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:658\u001b[0m, in \u001b[0;36mCarRacing._render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_image_array(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (VIDEO_W, VIDEO_H))\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_pixels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTATE_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTATE_H\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misopen\n",
      "File \u001b[1;32mc:\\Users\\Jimmy Walraff\\OneDrive - Universite de Liege\\Documents\\Ulg\\Master2\\RL\\Projet\\rlprojet\\lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:789\u001b[0m, in \u001b[0;36mCarRacing._create_image_array\u001b[1;34m(self, screen, size)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_image_array\u001b[39m(\u001b[38;5;28mself\u001b[39m, screen, size):\n\u001b[1;32m--> 789\u001b[0m     scaled_screen \u001b[38;5;241m=\u001b[39m \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmoothscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscreen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[0;32m    791\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(scaled_screen)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    792\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while global_step < TOTAL_TIMESTEPS:\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if LR_SCHEDULING == \"Linear\":\n",
    "        frac = 1.0 - global_step / TOTAL_TIMESTEPS\n",
    "        lrnow = frac * LEARNING_RATE\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, NB_STEPS):\n",
    "        global_step += NB_ENVS\n",
    "        obs[step] = next_obs\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "            current_logprobs[step] = logprob\n",
    "\n",
    "        actions[step] = action\n",
    "\n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "        \n",
    "        if \"episode\" in infos:\n",
    "            completed_episodes = infos[\"_episode\"]\n",
    "            episodic_returns = infos[\"episode\"][\"r\"][completed_episodes]\n",
    "            episodic_lengths = infos[\"episode\"][\"l\"][completed_episodes]\n",
    "\n",
    "            for episodic_return, episodic_length in zip(episodic_returns, episodic_lengths):\n",
    "                print(f\"global_step={global_step}, episodic_return={episodic_return}\")\n",
    "                if RUN_NAME != None:\n",
    "                    writer.add_scalar(\"charts/episodic_return\", episodic_return, global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", episodic_length, global_step)\n",
    "        \n",
    "        # Break when one of the environement as reached a terminal state\n",
    "        if torch.any(next_done):\n",
    "            break\n",
    "    \n",
    "    R = torch.Tensor([0 if next_done[i] == True else values[-1][i] for i in range(len(next_done))]).to(device)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "\n",
    "    for i in reversed(range(step)):\n",
    "        R = rewards[i] + GAMMA * R\n",
    "        returns[i] = R\n",
    "        advantages[i] = returns[i] - values[i]\n",
    "\n",
    "    # Normalize the advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    value_loss = torch.zeros((step, NB_ENVS))\n",
    "    actor_loss = torch.zeros((step, NB_ENVS))\n",
    "    entropy_term = torch.zeros((step, NB_ENVS))\n",
    "\n",
    "    for i in range(step):\n",
    "        _, logprob, ent, value = agent.get_action_and_value(obs[i], actions[i])\n",
    "        value = value.flatten()\n",
    "\n",
    "        actor_loss[i] = -logprob * advantages[i]\n",
    "        value_loss[i] = (returns[i] - value)**2\n",
    "        entropy_term[i] = ent\n",
    "\n",
    "    actor_loss = actor_loss.mean()\n",
    "    value_loss = 0.5 * value_loss.mean()\n",
    "    entropy_term = entropy_term.mean()\n",
    "\n",
    "    loss = actor_loss + VF_COEF * value_loss - ENT_COEF * entropy_term\n",
    "    approx_kl = (previous_logprobs - current_logprobs).mean() if global_step > NB_ENVS * NB_STEPS else 0.0\n",
    "\n",
    "    previous_logprobs = current_logprobs.detach().clone()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
    "    optimizer.step()\n",
    "\n",
    "    # logging for the losses + learning rate\n",
    "    if RUN_NAME != None:\n",
    "        writer.add_scalar(\"losses/total_loss\", loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/actor_loss\", actor_loss.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", value_loss.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_term.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl, global_step)\n",
    "        writer.add_scalar(\"charts/learning_rate\", lrnow, global_step)\n",
    "    \n",
    "    \n",
    "# Save the model at the end of training\n",
    "save_path = f\"trained_models/a2c/a2c_config{CONFIG_NUMBER}.pt\"\n",
    "os.makedirs(\"trained_models/a2c\", exist_ok=True)\n",
    "torch.save({\n",
    "    \"model_state_dict\": agent.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"config\": {\n",
    "        \"OBSERVATION_SIZE\": OBSERVATION_SIZE,\n",
    "        \"NB_FRAMES\": NB_FRAMES,\n",
    "        \"NB_ENVS\": NB_ENVS,\n",
    "        \"SEED\": SEED,\n",
    "        \"MAX_EPISODE_LENGTH\": MAX_EPISODE_LENGTH,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"NB_STEPS\": NB_STEPS,\n",
    "        \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,\n",
    "        \"LR_SCHEDULING\": LR_SCHEDULING,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"VF_COEF\": VF_COEF\n",
    "    }\n",
    "}, save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "if RUN_NAME != None:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlprojet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
