{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72d2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62bcd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "CONFIG_NUMBER = 13\n",
    "\n",
    "OBSERVATION_SIZE = 64\n",
    "NB_FRAMES = 1\n",
    "NB_ENVS = 8\n",
    "CUDA = False\n",
    "SEED = 2307\n",
    "\n",
    "MAX_EPISODE_LENGTH = 12000\n",
    "LEARNING_RATE = 1e-4\n",
    "NB_STEPS = 2048\n",
    "TOTAL_TIMESTEPS = 2e6\n",
    "RUN_NAME = \"A2C with GAE\"\n",
    "LR_SCHEDULING = \"Linear\"\n",
    "GAMMA = 0.99\n",
    "VF_COEF = 0.5\n",
    "ENT_COEF = 0.1\n",
    "MAX_GRAD_NORM = 0.5\n",
    "GAE_LAMBDA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73a7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def make_env():\n",
    "    def thunk():\n",
    "        env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.95, domain_randomize=False, continuous=True, max_episode_steps=MAX_EPISODE_LENGTH)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (OBSERVATION_SIZE, OBSERVATION_SIZE))\n",
    "        env = gym.wrappers.GrayscaleObservation(env)\n",
    "        env = gym.wrappers.FrameStackObservation(env, NB_FRAMES)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv([make_env() for _ in range(NB_ENVS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9867dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from TP5\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, nb_frames, image_size):\n",
    "        super(Agent, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.nb_frames = nb_frames\n",
    "\n",
    "        # Actor network\n",
    "        self.actor_network = self.build_network()\n",
    "        self.actor_mean = layer_init(\n",
    "            nn.Linear(512, np.prod(envs.single_action_space.shape)),\n",
    "            std=0.01\n",
    "        )\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))\n",
    "\n",
    "        # Critic network\n",
    "        self.critic_network = self.build_network()\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "\n",
    "    def build_network(self):\n",
    "        stride = [4, 2, 1]\n",
    "        kernel_size = [8, 4, 3]\n",
    "        input_channels = [self.nb_frames, 32, 64]\n",
    "        output_channels = [32, 64, 64]\n",
    "        image_size = self.image_size\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(stride)):\n",
    "            layers.append(layer_init(nn.Conv2d(input_channels[i], output_channels[i], kernel_size[i], stride=stride[i])))\n",
    "            layers.append(nn.ReLU())\n",
    "            image_size = math.floor(((image_size - kernel_size[i]) / stride[i]) + 1)\n",
    "\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(layer_init(nn.Linear(output_channels[-1] * image_size * image_size, 512)))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        hidden = self.critic_network(x / 255.0)\n",
    "        return self.critic(hidden)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        actor_hidden = self.actor_network(x / 255.0)\n",
    "        action_mean = self.actor_mean(actor_hidden)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        critic_hidden = self.critic_network(x / 255.0)\n",
    "        value = self.critic(critic_hidden)\n",
    "\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468837cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflo230702\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.7s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Session/Documents/Universite/Master 2/Reinforcement learning/Project/wandb/run-20250429_220436-58toj7e8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/58toj7e8' target=\"_blank\">Fifth Config A2C</a></strong> to <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/58toj7e8' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project/runs/58toj7e8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and CUDA else \"cpu\")\n",
    "agent = Agent(envs, NB_FRAMES, OBSERVATION_SIZE).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "obs = torch.zeros((NB_STEPS, NB_ENVS) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((NB_STEPS, NB_ENVS) + envs.single_action_space.shape).to(device)\n",
    "rewards = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "values = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "current_logprobs = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "previous_logprobs = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "\n",
    "global_step = 0\n",
    "next_obs, _ = envs.reset(seed=SEED)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(NB_ENVS).to(device)\n",
    "\n",
    "if RUN_NAME is not None:\n",
    "    import wandb\n",
    "\n",
    "    config = {\n",
    "        \"OBSERVATION_SIZE\": OBSERVATION_SIZE,\n",
    "        \"NB_FRAMES\": NB_FRAMES,\n",
    "        \"NB_ENVS\": NB_ENVS,\n",
    "        \"SEED\": SEED,\n",
    "        \"MAX_EPISODE_LENGTH\": MAX_EPISODE_LENGTH,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"NB_STEPS\": NB_STEPS,\n",
    "        \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,\n",
    "        \"LR_SCHEDULING\": LR_SCHEDULING,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"VF_COEF\": VF_COEF,\n",
    "        \"CUDA\": CUDA,\n",
    "        \"ENT_COEF\": ENT_COEF,\n",
    "        \"CONFIG_NUMBER\": CONFIG_NUMBER,\n",
    "        \"GAE_LAMBDA\": GAE_LAMBDA\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        entity=\"Rl2025-project\",\n",
    "        project=\"RL Project\",\n",
    "        name=RUN_NAME,\n",
    "        config=config,\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "        monitor_gym=True,       # auto-upload des vidéos de l'agent\n",
    "        # save_code=True,       # optionnel\n",
    "    )\n",
    "    \n",
    "    writer = SummaryWriter(f\"runs/{RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadde90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=63256, episodic_return=-785.5275362319693\n",
      "global_step=91336, episodic_return=-1182.109915014275\n",
      "global_step=94368, episodic_return=-1209.0774647888188\n",
      "global_step=94520, episodic_return=-1206.131182795788\n",
      "global_step=95648, episodic_return=-1217.2687074830717\n",
      "global_step=96000, episodic_return=-1120.5776173285951\n",
      "global_step=96000, episodic_return=-1120.1277955272317\n",
      "global_step=96000, episodic_return=-1129.0322580645836\n",
      "global_step=150944, episodic_return=-1129.0306990883046\n",
      "global_step=152280, episodic_return=-744.8769230770008\n",
      "global_step=155168, episodic_return=-818.6607594937659\n",
      "global_step=181472, episodic_return=-1159.6210191084117\n",
      "global_step=181600, episodic_return=-1082.6787878789428\n",
      "global_step=183648, episodic_return=-1118.444067796772\n",
      "global_step=186368, episodic_return=-1143.378494623802\n",
      "global_step=192008, episodic_return=-1132.8859060403329\n",
      "global_step=195264, episodic_return=-576.4939393939796\n",
      "global_step=225464, episodic_return=-558.397435897472\n",
      "global_step=226872, episodic_return=-450.63781512605937\n",
      "global_step=227080, episodic_return=-985.2748344372192\n",
      "global_step=228872, episodic_return=-952.1344827587459\n",
      "global_step=254048, episodic_return=-933.3157894738057\n",
      "global_step=254336, episodic_return=-920.8773946361339\n",
      "global_step=258024, episodic_return=-819.3649350650302\n",
      "global_step=264984, episodic_return=-935.3716475097006\n",
      "global_step=267680, episodic_return=-557.8166666667025\n",
      "global_step=269160, episodic_return=-533.4699300699606\n",
      "global_step=270808, episodic_return=-621.3038327526632\n",
      "global_step=289640, episodic_return=-817.4913183280693\n",
      "global_step=297720, episodic_return=-525.1788273615899\n",
      "global_step=301672, episodic_return=-497.99731543626393\n",
      "global_step=306072, episodic_return=-444.0125000000096\n",
      "global_step=306352, episodic_return=-681.372549019672\n",
      "global_step=312616, episodic_return=-551.9302013423166\n",
      "global_step=315800, episodic_return=-752.9309309310115\n",
      "global_step=324808, episodic_return=-487.451948051967\n",
      "global_step=325440, episodic_return=-752.1797468355234\n",
      "global_step=333272, episodic_return=-341.31515151514213\n",
      "global_step=334384, episodic_return=-398.1166666666659\n",
      "global_step=336464, episodic_return=-483.05470383277054\n",
      "global_step=350360, episodic_return=-644.179816513817\n",
      "global_step=359048, episodic_return=-601.0633093525632\n",
      "global_step=411808, episodic_return=-1099.3506493507523\n",
      "global_step=420816, episodic_return=-1072.8323699423336\n",
      "global_step=421448, episodic_return=-1107.5342465754243\n",
      "global_step=429280, episodic_return=-1100.3558718862087\n",
      "global_step=430392, episodic_return=-1025.726141079007\n",
      "global_step=432472, episodic_return=-1021.9178082193665\n",
      "global_step=446368, episodic_return=-1085.24590163946\n",
      "global_step=455056, episodic_return=-1077.9527559056116\n",
      "global_step=507816, episodic_return=-996.6101694917057\n",
      "global_step=516824, episodic_return=-965.1245551603217\n",
      "global_step=517456, episodic_return=-1128.358208955294\n",
      "global_step=525288, episodic_return=-989.8550724639597\n",
      "global_step=526400, episodic_return=-1040.304182509668\n",
      "global_step=528480, episodic_return=-1046.9055374594443\n",
      "global_step=538904, episodic_return=-256.8408921933052\n",
      "global_step=542376, episodic_return=-1019.4945848377397\n",
      "global_step=551064, episodic_return=-1057.6271186442218\n",
      "global_step=553640, episodic_return=-540.3419354839054\n",
      "global_step=564592, episodic_return=-337.86750902526205\n",
      "global_step=569808, episodic_return=-539.988039867144\n",
      "global_step=571520, episodic_return=-217.1906752411549\n",
      "global_step=574960, episodic_return=-655.5179487180067\n",
      "global_step=588352, episodic_return=-237.02926829267963\n",
      "global_step=589408, episodic_return=-635.0684210526849\n",
      "global_step=590912, episodic_return=-863.9202247192121\n",
      "global_step=593064, episodic_return=-854.736065573887\n",
      "global_step=602088, episodic_return=-176.56293706293738\n",
      "global_step=603416, episodic_return=-423.7539682539733\n",
      "global_step=608624, episodic_return=-157.1944444444449\n",
      "global_step=611664, episodic_return=-339.7834394904364\n",
      "global_step=614816, episodic_return=-615.4201342282365\n",
      "global_step=620224, episodic_return=-327.01929824560636\n",
      "global_step=623160, episodic_return=-207.8916913946568\n",
      "global_step=625688, episodic_return=-272.9662207357804\n",
      "global_step=632216, episodic_return=-236.1191881918788\n",
      "global_step=633272, episodic_return=-379.36704119849367\n",
      "global_step=635064, episodic_return=-173.5146757679184\n",
      "global_step=635304, episodic_return=-613.5292682927314\n",
      "global_step=636848, episodic_return=-233.275675675673\n",
      "global_step=647072, episodic_return=-944.6808510640102\n",
      "global_step=649416, episodic_return=-236.4534201954367\n",
      "global_step=652312, episodic_return=-145.30000000000032\n",
      "global_step=658048, episodic_return=-1111.2055749130573\n",
      "global_step=662640, episodic_return=-214.74028776978216\n",
      "global_step=666296, episodic_return=-168.65342465753469\n",
      "global_step=671712, episodic_return=-177.48571428571427\n",
      "global_step=687840, episodic_return=-271.59867549668326\n",
      "global_step=690032, episodic_return=-662.4942492013469\n",
      "global_step=693032, episodic_return=-238.87804878048655\n",
      "global_step=695456, episodic_return=-125.66451612903248\n",
      "global_step=698528, episodic_return=-818.9439024391373\n",
      "global_step=701920, episodic_return=-923.0071672356138\n",
      "global_step=703840, episodic_return=-747.9615384616225\n",
      "global_step=705544, episodic_return=-151.19636963696428\n",
      "global_step=705808, episodic_return=-289.0643109540573\n",
      "global_step=710896, episodic_return=-95.65806451612963\n",
      "global_step=713728, episodic_return=-168.35766871165697\n",
      "global_step=718912, episodic_return=-741.0649402391223\n",
      "global_step=720216, episodic_return=-813.380144404433\n",
      "global_step=723664, episodic_return=-323.9277777777669\n",
      "global_step=732384, episodic_return=-198.56666666666564\n",
      "global_step=733640, episodic_return=-298.18934707902895\n",
      "global_step=733976, episodic_return=-165.3197183098598\n",
      "global_step=737616, episodic_return=-364.1052631578876\n",
      "global_step=745768, episodic_return=-150.22941176470653\n",
      "global_step=746384, episodic_return=-289.5949843260116\n",
      "global_step=749184, episodic_return=-176.6396825396837\n",
      "global_step=756560, episodic_return=-163.80297397769635\n",
      "global_step=758840, episodic_return=-188.55111821086194\n",
      "global_step=763248, episodic_return=-106.28888888888925\n",
      "global_step=769712, episodic_return=-134.33665480427098\n",
      "global_step=789040, episodic_return=-1000.6644518274007\n",
      "global_step=797928, episodic_return=-1002.9739776953353\n",
      "global_step=801080, episodic_return=-422.05873015873505\n",
      "global_step=809736, episodic_return=-965.7342657344445\n",
      "global_step=815648, episodic_return=-733.3275362319589\n",
      "global_step=815824, episodic_return=-877.7082018928535\n",
      "global_step=828264, episodic_return=-210.4282051282023\n",
      "global_step=828392, episodic_return=-1014.2414860682721\n",
      "global_step=829984, episodic_return=-1015.0943396228132\n",
      "global_step=851336, episodic_return=-620.2413793103952\n",
      "global_step=876240, episodic_return=-1066.7841269842872\n",
      "global_step=893936, episodic_return=-1009.8859315591071\n",
      "global_step=905744, episodic_return=-1059.8484848486182\n",
      "global_step=911656, episodic_return=-1020.5128205130184\n",
      "global_step=924272, episodic_return=-1131.1475409836708\n",
      "global_step=924400, episodic_return=-1070.5035971224297\n",
      "global_step=925992, episodic_return=-994.3262411349351\n",
      "global_step=947344, episodic_return=-1040.425531915067\n",
      "global_step=972248, episodic_return=-1046.6666666668075\n",
      "global_step=987504, episodic_return=-996.3980707397037\n",
      "global_step=989944, episodic_return=-1019.1489361703989\n",
      "global_step=1007664, episodic_return=-1065.1685393259863\n",
      "global_step=1020280, episodic_return=-1056.6552901025352\n",
      "global_step=1020408, episodic_return=-1052.5641025642626\n",
      "global_step=1022000, episodic_return=-1041.4634146343167\n",
      "global_step=1043352, episodic_return=-1046.1538461540035\n",
      "global_step=1060976, episodic_return=-784.853024911139\n",
      "global_step=1068256, episodic_return=-1093.103448275959\n",
      "global_step=1083512, episodic_return=-1091.608391608505\n",
      "global_step=1084392, episodic_return=-535.1300970874083\n",
      "global_step=1086680, episodic_return=-876.2715302492198\n",
      "global_step=1103672, episodic_return=-1086.7924528302908\n",
      "global_step=1116288, episodic_return=-1132.2580645161947\n",
      "global_step=1116416, episodic_return=-1032.1678321680024\n",
      "global_step=1128824, episodic_return=-439.87800829875556\n",
      "global_step=1156984, episodic_return=-1054.8895899055126\n",
      "global_step=1164264, episodic_return=-1133.070866141808\n",
      "global_step=1179520, episodic_return=-1103.1250000000903\n",
      "global_step=1180400, episodic_return=-1092.6174496645383\n",
      "global_step=1185368, episodic_return=-770.2963696370475\n",
      "global_step=1199680, episodic_return=-1109.0909090909936\n",
      "global_step=1212296, episodic_return=-1103.1007751938864\n",
      "global_step=1212424, episodic_return=-1053.658536585501\n",
      "global_step=1252992, episodic_return=-1129.1044776120093\n",
      "global_step=1260272, episodic_return=-1043.0034129694334\n",
      "global_step=1275528, episodic_return=-1027.8388278389843\n",
      "global_step=1276408, episodic_return=-1111.1969111970106\n",
      "global_step=1281376, episodic_return=-1129.629629629705\n",
      "global_step=1295688, episodic_return=-1071.7105263159233\n",
      "global_step=1308304, episodic_return=-1143.3333333333883\n",
      "global_step=1308432, episodic_return=-988.2736156353635\n",
      "global_step=1349000, episodic_return=-1051.7241379311788\n",
      "global_step=1356280, episodic_return=-1049.3150684933078\n",
      "global_step=1371536, episodic_return=-1057.1428571430051\n",
      "global_step=1372416, episodic_return=-1109.7472924188735\n",
      "global_step=1377384, episodic_return=-1108.4745762712687\n",
      "global_step=1391696, episodic_return=-1088.05970149265\n",
      "global_step=1404312, episodic_return=-1044.5229681980459\n",
      "global_step=1404440, episodic_return=-1091.7197452230446\n",
      "global_step=1437112, episodic_return=-475.95384615386337\n",
      "global_step=1445008, episodic_return=-990.0355871887697\n",
      "global_step=1452288, episodic_return=-1080.1369863014788\n",
      "global_step=1467544, episodic_return=-1151.0489510490038\n",
      "global_step=1468424, episodic_return=-1067.38351254493\n",
      "global_step=1473392, episodic_return=-1123.8754325260304\n",
      "global_step=1487704, episodic_return=-1095.1048951050095\n",
      "global_step=1500448, episodic_return=-1007.8175895767336\n",
      "global_step=1533120, episodic_return=-1088.8888888889805\n",
      "global_step=1541016, episodic_return=-1166.6666666667013\n",
      "global_step=1544576, episodic_return=-759.3404833837662\n",
      "global_step=1548296, episodic_return=-1122.261484099018\n",
      "global_step=1563552, episodic_return=-1107.534246575441\n",
      "global_step=1564432, episodic_return=-1129.3286219081983\n",
      "global_step=1569400, episodic_return=-1020.8955223882449\n",
      "global_step=1596456, episodic_return=-998.1072555206837\n",
      "global_step=1613576, episodic_return=-974.7307692309065\n",
      "global_step=1637024, episodic_return=-1007.9710144929301\n",
      "global_step=1640584, episodic_return=-1085.7142857143522\n",
      "global_step=1644304, episodic_return=-1022.5806451614669\n",
      "global_step=1659560, episodic_return=-1070.3703703704869\n",
      "global_step=1660440, episodic_return=-1012.7340823971987\n",
      "global_step=1664640, episodic_return=-626.6135135135673\n",
      "global_step=1665408, episodic_return=-973.4375000001529\n",
      "global_step=1692464, episodic_return=-1066.6666666668023\n",
      "global_step=1731432, episodic_return=-812.66610169501\n",
      "global_step=1733032, episodic_return=-922.7722772279\n",
      "global_step=1736592, episodic_return=-994.4785276075378\n",
      "global_step=1740312, episodic_return=-954.8387096775992\n",
      "global_step=1755568, episodic_return=-1000.6042296074479\n",
      "global_step=1756448, episodic_return=-886.8327402137057\n",
      "global_step=1761416, episodic_return=-1083.9590443687205\n",
      "global_step=1777840, episodic_return=-329.75709342559605\n",
      "global_step=1788472, episodic_return=-1021.9178082193669\n",
      "global_step=1827440, episodic_return=-1041.328413284279\n",
      "global_step=1829040, episodic_return=-973.722627737412\n",
      "global_step=1832600, episodic_return=-1079.310344827722\n",
      "global_step=1835520, episodic_return=-1118.258208955419\n",
      "global_step=1852456, episodic_return=-1049.0566037737367\n",
      "global_step=1857424, episodic_return=-1011.6438356166349\n",
      "global_step=1873848, episodic_return=-1064.1509433963645\n",
      "global_step=1874280, episodic_return=-1104.8324324325918\n",
      "global_step=1895192, episodic_return=-785.9267080746248\n",
      "global_step=1923448, episodic_return=-1057.6388888889728\n",
      "global_step=1925048, episodic_return=-1099.7050147493537\n",
      "global_step=1931528, episodic_return=-1066.6666666668025\n",
      "global_step=1948464, episodic_return=-1041.3793103449977\n",
      "global_step=1953432, episodic_return=-1101.1627906977785\n",
      "global_step=1969856, episodic_return=-1035.6643356644915\n",
      "global_step=1970288, episodic_return=-1007.27272727292\n",
      "global_step=1991200, episodic_return=-1058.1081081082548\n",
      "global_step=2007080, episodic_return=-681.4320284698148\n",
      "Model saved to trained_models/a2c/a2c_config5.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>charts/episodic_length</td><td>▅██▇▆▃▄▄█▁▅▃▁▆▇▂▁█▃████▄████▅███████████</td></tr><tr><td>charts/episodic_return</td><td>▁▂▁▆▃▅▆▄▇▆▂▂▇▂▂▅▃▇█▃▄▇█▆▇▂▂▂▂▆▆▂▁▂▂▂▂▇▂▄</td></tr><tr><td>charts/learning_rate</td><td>██████▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▅▅▄▄▄▄▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>losses/actor_loss</td><td>▅▃▁▅▁▅▅▅▅▅▄▅▅▄▅▅▄▄▅▅▅▅▅█▅▅▅▅▅▅▄▅▅▄▅▆▅▅▅▅</td></tr><tr><td>losses/approx_kl</td><td>▇▅▂▅▅▅▄▅▄▄▄▅▇▅▄▆▅▆█▅▅▃▆▄▃▆▂▄▄▆▃▆▇▅▄▆▄▁▅▅</td></tr><tr><td>losses/entropy</td><td>███▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>losses/total_loss</td><td>▃▆▂▂▃▂▂▂▂▂▄▂▂▃▃▁▂▂▂▃▂▂▂▁█▃▂▂▅▂▇▂▃▂▂▂▂▂▂▂</td></tr><tr><td>losses/value_loss</td><td>▃▂▂▂▃▂▁▁▁▆▁▂▂▃▃▅▂▃▄▂▄▄█▂▁▂▁▁▁█▃▂▂▂▂▁▂▃▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>charts/episodic_length</td><td>6705</td></tr><tr><td>charts/episodic_return</td><td>-681.432</td></tr><tr><td>charts/learning_rate</td><td>0.0</td></tr><tr><td>global_step</td><td>2007080</td></tr><tr><td>losses/actor_loss</td><td>-0.00578</td></tr><tr><td>losses/approx_kl</td><td>-0.009</td></tr><tr><td>losses/entropy</td><td>4.23278</td></tr><tr><td>losses/total_loss</td><td>2.14081</td></tr><tr><td>losses/value_loss</td><td>3.44661</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Fifth Config A2C</strong> at: <a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/58toj7e8' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project/runs/58toj7e8</a><br> View project at: <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250429_220436-58toj7e8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "while global_step < TOTAL_TIMESTEPS:\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if LR_SCHEDULING == \"Linear\":\n",
    "        frac = 1.0 - global_step / TOTAL_TIMESTEPS\n",
    "        lrnow = frac * LEARNING_RATE\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, NB_STEPS):\n",
    "        global_step += NB_ENVS\n",
    "        obs[step] = next_obs\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "            current_logprobs[step] = logprob\n",
    "\n",
    "        actions[step] = action\n",
    "\n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "        \n",
    "        if \"episode\" in infos:\n",
    "            completed_episodes = infos[\"_episode\"]\n",
    "            episodic_returns = infos[\"episode\"][\"r\"][completed_episodes]\n",
    "            episodic_lengths = infos[\"episode\"][\"l\"][completed_episodes]\n",
    "\n",
    "            for episodic_return, episodic_length in zip(episodic_returns, episodic_lengths):\n",
    "                print(f\"global_step={global_step}, episodic_return={episodic_return}\")\n",
    "                if RUN_NAME != None:\n",
    "                    writer.add_scalar(\"charts/episodic_return\", episodic_return, global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", episodic_length, global_step)\n",
    "        \n",
    "        # Break when one of the environement as reached a terminal state\n",
    "        if torch.any(next_done):\n",
    "            break\n",
    "    \n",
    "    if GAE_LAMBDA == None:\n",
    "        R = torch.Tensor([0 if next_done[i] == True else values[-1][i] for i in range(len(next_done))]).to(device)\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "\n",
    "        for i in reversed(range(step)):\n",
    "            R = rewards[i] + GAMMA * R\n",
    "            returns[i] = R\n",
    "            advantages[i] = returns[i] - values[i]\n",
    "    \n",
    "    else:\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        lastgaelam = 0\n",
    "\n",
    "        for t in reversed(range(step)):\n",
    "            if t == step - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalue = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - torch.zeros_like(next_done)\n",
    "                nextvalue = values[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + GAMMA * nextvalue * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + GAMMA * GAE_LAMBDA * nextnonterminal * lastgaelam\n",
    "\n",
    "        returns = advantages + values\n",
    "\n",
    "    # Normalize the advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    value_loss = torch.zeros((step, NB_ENVS))\n",
    "    actor_loss = torch.zeros((step, NB_ENVS))\n",
    "    entropy_term = torch.zeros((step, NB_ENVS))\n",
    "\n",
    "    for i in range(step):\n",
    "        _, logprob, ent, value = agent.get_action_and_value(obs[i], actions[i])\n",
    "        value = value.flatten()\n",
    "\n",
    "        actor_loss[i] = -logprob * advantages[i]\n",
    "        value_loss[i] = (returns[i] - value)**2\n",
    "        entropy_term[i] = ent\n",
    "\n",
    "    actor_loss = actor_loss.mean()\n",
    "    value_loss = 0.5 * value_loss.mean()\n",
    "    entropy_term = entropy_term.mean()\n",
    "\n",
    "    loss = actor_loss + VF_COEF * value_loss + ENT_COEF * entropy_term\n",
    "    approx_kl = (previous_logprobs - current_logprobs).mean() if global_step > NB_ENVS * NB_STEPS else 0.0\n",
    "\n",
    "    previous_logprobs = current_logprobs.detach().clone()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
    "    optimizer.step()\n",
    "\n",
    "    # logging for the losses + learning rate\n",
    "    if RUN_NAME != None:\n",
    "        writer.add_scalar(\"losses/total_loss\", loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/actor_loss\", actor_loss.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", value_loss.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_term.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl, global_step)\n",
    "        writer.add_scalar(\"charts/learning_rate\", lrnow, global_step)\n",
    "    \n",
    "    \n",
    "# Save the model at the end of training\n",
    "save_path = f\"trained_models/a2c/a2c_config{CONFIG_NUMBER}.pt\"\n",
    "os.makedirs(\"trained_models/a2c\", exist_ok=True)\n",
    "torch.save({\n",
    "    \"model_state_dict\": agent.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"config\": {\n",
    "        \"OBSERVATION_SIZE\": OBSERVATION_SIZE,\n",
    "        \"NB_FRAMES\": NB_FRAMES,\n",
    "        \"NB_ENVS\": NB_ENVS,\n",
    "        \"SEED\": SEED,\n",
    "        \"MAX_EPISODE_LENGTH\": MAX_EPISODE_LENGTH,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"NB_STEPS\": NB_STEPS,\n",
    "        \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,\n",
    "        \"LR_SCHEDULING\": LR_SCHEDULING,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"VF_COEF\": VF_COEF\n",
    "    }\n",
    "}, save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "if RUN_NAME != None:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_racing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
