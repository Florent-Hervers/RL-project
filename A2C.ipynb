{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72d2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62bcd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "CONFIG_NUMBER = 4\n",
    "\n",
    "OBSERVATION_SIZE = 84\n",
    "NB_FRAMES = 4\n",
    "NB_ENVS = 1\n",
    "CUDA = False\n",
    "SEED = 2307\n",
    "\n",
    "MAX_EPISODE_LENGTH = 12000\n",
    "LEARNING_RATE = 1e-4\n",
    "NB_STEPS = 256\n",
    "TOTAL_TIMESTEPS = 2e6\n",
    "RUN_NAME = \"Fourth Config A2C with no entropy and updated value loss\"\n",
    "LR_SCHEDULING = \"Linear\"\n",
    "GAMMA = 0.99\n",
    "VF_COEF = 0.5\n",
    "ENT_COEF = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73a7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def make_env():\n",
    "    def thunk():\n",
    "        env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.95, domain_randomize=False, continuous=True, max_episode_steps=MAX_EPISODE_LENGTH)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (OBSERVATION_SIZE, OBSERVATION_SIZE))\n",
    "        env = gym.wrappers.GrayscaleObservation(env)\n",
    "        env = gym.wrappers.FrameStackObservation(env, NB_FRAMES)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv([make_env() for _ in range(NB_ENVS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9867dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from TP5\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, nb_frames, image_size):\n",
    "        super(Agent, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.nb_frames = nb_frames\n",
    "\n",
    "        # Actor network\n",
    "        self.actor_network = self.build_network()\n",
    "        self.actor_mean = layer_init(\n",
    "            nn.Linear(512, np.prod(envs.single_action_space.shape)),\n",
    "            std=0.01\n",
    "        )\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))\n",
    "\n",
    "        # Critic network\n",
    "        self.critic_network = self.build_network()\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "\n",
    "    def build_network(self):\n",
    "        stride = [4, 2, 1]\n",
    "        kernel_size = [8, 4, 3]\n",
    "        input_channels = [self.nb_frames, 32, 64]\n",
    "        output_channels = [32, 64, 64]\n",
    "        image_size = self.image_size\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(stride)):\n",
    "            layers.append(layer_init(nn.Conv2d(input_channels[i], output_channels[i], kernel_size[i], stride=stride[i])))\n",
    "            layers.append(nn.ReLU())\n",
    "            image_size = math.floor(((image_size - kernel_size[i]) / stride[i]) + 1)\n",
    "\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(layer_init(nn.Linear(output_channels[-1] * image_size * image_size, 512)))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        hidden = self.critic_network(x / 255.0)\n",
    "        return self.critic(hidden)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        actor_hidden = self.actor_network(x / 255.0)\n",
    "        action_mean = self.actor_mean(actor_hidden)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        critic_hidden = self.critic_network(x / 255.0)\n",
    "        value = self.critic(critic_hidden)\n",
    "\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468837cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflo230702\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.7s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Session/Documents/Universite/Master 2/Reinforcement learning/Project/wandb/run-20250428_223530-27p2cdh9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/27p2cdh9' target=\"_blank\">Fourth Config A2C with no entropy and updated value loss</a></strong> to <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/27p2cdh9' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project/runs/27p2cdh9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and CUDA else \"cpu\")\n",
    "agent = Agent(envs, NB_FRAMES, OBSERVATION_SIZE).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "obs = torch.zeros((NB_STEPS, NB_ENVS) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((NB_STEPS, NB_ENVS) + envs.single_action_space.shape).to(device)\n",
    "rewards = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "values = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "current_logprobs = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "previous_logprobs = torch.zeros((NB_STEPS, NB_ENVS)).to(device)\n",
    "\n",
    "global_step = 0\n",
    "next_obs, _ = envs.reset(seed=SEED)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(NB_ENVS).to(device)\n",
    "\n",
    "if RUN_NAME is not None:\n",
    "    import wandb\n",
    "\n",
    "    config = {\n",
    "        \"OBSERVATION_SIZE\": OBSERVATION_SIZE,\n",
    "        \"NB_FRAMES\": NB_FRAMES,\n",
    "        \"NB_ENVS\": NB_ENVS,\n",
    "        \"SEED\": SEED,\n",
    "        \"MAX_EPISODE_LENGTH\": MAX_EPISODE_LENGTH,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"NB_STEPS\": NB_STEPS,\n",
    "        \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,\n",
    "        \"LR_SCHEDULING\": LR_SCHEDULING,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"VF_COEF\": VF_COEF,\n",
    "        \"CUDA\": CUDA,\n",
    "        \"ENT_COEF\": ENT_COEF,\n",
    "        \"CONFIG_NUMBER\": CONFIG_NUMBER\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        entity=\"Rl2025-project\",\n",
    "        project=\"RL Project\",\n",
    "        name=RUN_NAME,\n",
    "        config=config,\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "        monitor_gym=True,       # auto-upload des vidéos de l'agent\n",
    "        # save_code=True,       # optionnel\n",
    "    )\n",
    "    \n",
    "    writer = SummaryWriter(f\"runs/{RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadde90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=9631, episodic_return=-986.5127478754855\n",
      "global_step=21632, episodic_return=-1133.1210191083458\n",
      "global_step=33633, episodic_return=-1129.8245614035761\n",
      "global_step=41937, episodic_return=-838.9947882737177\n",
      "global_step=48978, episodic_return=-629.6839721254812\n",
      "global_step=60979, episodic_return=-1056.164383561778\n",
      "global_step=72980, episodic_return=-1141.368078175954\n",
      "global_step=79104, episodic_return=-662.3661129568696\n",
      "global_step=91105, episodic_return=-1116.1073825504143\n",
      "global_step=103106, episodic_return=-1181.549815498176\n",
      "global_step=115107, episodic_return=-1184.7560975609938\n",
      "global_step=127108, episodic_return=-1183.4983498350027\n",
      "global_step=139109, episodic_return=-1184.6625766871348\n",
      "global_step=151110, episodic_return=-1182.5174825175027\n",
      "global_step=163111, episodic_return=-1184.848484848503\n",
      "global_step=175112, episodic_return=-1183.9228295820124\n",
      "global_step=187113, episodic_return=-1186.013986014003\n",
      "global_step=199114, episodic_return=-1184.3750000000184\n",
      "global_step=211115, episodic_return=-1181.6849816850026\n",
      "global_step=223116, episodic_return=-1183.7662337662528\n",
      "global_step=235117, episodic_return=-1182.5174825175027\n",
      "global_step=247118, episodic_return=-1182.8767123287869\n",
      "global_step=259119, episodic_return=-1177.7777777778022\n",
      "global_step=271120, episodic_return=-1160.7250755287412\n",
      "global_step=283121, episodic_return=-1182.6989619377362\n",
      "global_step=295122, episodic_return=-1181.1320754717194\n",
      "global_step=307123, episodic_return=-1182.5174825175027\n",
      "global_step=319124, episodic_return=-1185.6733524355473\n",
      "global_step=331125, episodic_return=-1165.156794425127\n",
      "global_step=343126, episodic_return=-1166.9117647059168\n",
      "global_step=355127, episodic_return=-1171.2230215827642\n",
      "global_step=359652, episodic_return=-534.9989619377466\n",
      "global_step=371653, episodic_return=-1100.79365079374\n",
      "global_step=383654, episodic_return=-1158.1881533101337\n",
      "global_step=386596, episodic_return=-368.2120343839466\n",
      "global_step=398597, episodic_return=-1042.293906810205\n",
      "global_step=410598, episodic_return=-1131.7406143345374\n",
      "global_step=422599, episodic_return=-977.3279352228594\n",
      "global_step=434600, episodic_return=-960.7142857144629\n",
      "global_step=437205, episodic_return=-314.1461538461461\n",
      "global_step=449206, episodic_return=-1144.9838187702796\n",
      "global_step=461207, episodic_return=-1012.5000000001668\n",
      "global_step=470206, episodic_return=-910.6589041097066\n",
      "global_step=482207, episodic_return=-1082.9431438128288\n",
      "global_step=494208, episodic_return=-910.8843537416828\n",
      "global_step=506209, episodic_return=-846.0481099658127\n",
      "global_step=513354, episodic_return=-547.1767123288065\n",
      "global_step=519336, episodic_return=-537.0410958904412\n",
      "global_step=521628, episodic_return=-237.4745762711798\n",
      "global_step=526796, episodic_return=-482.5003831417822\n",
      "global_step=528460, episodic_return=-169.2100334448172\n",
      "global_step=540461, episodic_return=-1116.1290322581442\n",
      "global_step=552462, episodic_return=-1181.1320754717194\n",
      "global_step=564463, episodic_return=-1157.7039274924948\n",
      "global_step=576464, episodic_return=-1173.5099337748625\n",
      "global_step=588465, episodic_return=-1095.3846153847107\n",
      "global_step=600466, episodic_return=-997.9094076656967\n",
      "global_step=612467, episodic_return=-1025.949367088771\n",
      "global_step=624468, episodic_return=-816.4874551973188\n",
      "global_step=636469, episodic_return=-1070.9677419356071\n",
      "global_step=648470, episodic_return=-1097.8723404256202\n",
      "global_step=660471, episodic_return=-1119.0140845071182\n",
      "global_step=672472, episodic_return=-1055.555555555699\n",
      "global_step=679739, episodic_return=-715.7733564014595\n",
      "global_step=689541, episodic_return=-891.3772455091206\n",
      "global_step=693696, episodic_return=-385.55316455695817\n",
      "global_step=695240, episodic_return=-136.95862068965445\n",
      "global_step=696509, episodic_return=-181.40383275261317\n",
      "global_step=697776, episodic_return=-182.3232931726906\n",
      "global_step=700862, episodic_return=-226.58181818181555\n",
      "global_step=707593, episodic_return=-565.6272727273089\n",
      "global_step=708885, episodic_return=-122.83561643835617\n",
      "global_step=712079, episodic_return=-332.869064748189\n",
      "global_step=713778, episodic_return=-226.22173913043193\n",
      "global_step=716623, episodic_return=-271.5727272727229\n",
      "global_step=717665, episodic_return=-157.98773006135036\n",
      "global_step=719447, episodic_return=-199.23287671232802\n",
      "global_step=726979, episodic_return=-592.8648648649587\n",
      "global_step=728201, episodic_return=-199.69516728624393\n",
      "global_step=740202, episodic_return=-794.202898550918\n",
      "global_step=742136, episodic_return=-202.54844192634428\n",
      "global_step=742792, episodic_return=-127.14863387978168\n",
      "global_step=754793, episodic_return=-809.1205211727669\n",
      "global_step=766794, episodic_return=-1005.9701492539183\n",
      "global_step=769646, episodic_return=-254.09090909090517\n",
      "global_step=781647, episodic_return=-1067.8571428572684\n",
      "global_step=793648, episodic_return=-1175.5351681957452\n",
      "global_step=805649, episodic_return=-1154.3859649123267\n",
      "global_step=817111, episodic_return=-1211.732087227499\n",
      "global_step=829112, episodic_return=-955.7251908398405\n",
      "global_step=834375, episodic_return=-584.4333333333752\n",
      "global_step=846376, episodic_return=-1151.2195121951715\n",
      "global_step=858377, episodic_return=-1180.132450331148\n",
      "global_step=863550, episodic_return=-584.0117647059241\n",
      "global_step=866580, episodic_return=-313.75890410958107\n",
      "global_step=876124, episodic_return=-1026.030985915635\n",
      "global_step=880167, episodic_return=-412.26326530612477\n",
      "global_step=881728, episodic_return=-233.09869706840055\n",
      "global_step=883446, episodic_return=-238.70526315789112\n",
      "global_step=889152, episodic_return=-533.5284916201421\n",
      "global_step=889865, episodic_return=-112.08360655737738\n",
      "global_step=901866, episodic_return=-919.8697068405806\n",
      "global_step=913867, episodic_return=-1142.9657794677369\n",
      "global_step=916806, episodic_return=-301.2342465753353\n",
      "global_step=917670, episodic_return=-107.54831460674227\n",
      "global_step=919489, episodic_return=-245.03333333332952\n",
      "global_step=921024, episodic_return=-175.5614840989407\n",
      "global_step=921392, episodic_return=-104.79787985865725\n",
      "global_step=923794, episodic_return=-238.69281045751305\n",
      "global_step=925489, episodic_return=-232.93636363636054\n",
      "global_step=925998, episodic_return=-110.69999999999999\n",
      "global_step=927166, episodic_return=-134.70344827586297\n",
      "global_step=927467, episodic_return=-102.0115537848606\n",
      "global_step=933622, episodic_return=-521.1823529412309\n",
      "global_step=939512, episodic_return=-418.5297297297242\n",
      "global_step=940929, episodic_return=-172.4210526315793\n",
      "global_step=942438, episodic_return=-192.67952218429951\n",
      "global_step=946228, episodic_return=-332.24827586205663\n",
      "global_step=947210, episodic_return=-142.0559440559444\n",
      "global_step=950351, episodic_return=-287.317721518981\n",
      "global_step=952990, episodic_return=-285.99729729729165\n",
      "global_step=953225, episodic_return=-100.93578274760377\n",
      "global_step=953670, episodic_return=-93.66708860759529\n",
      "global_step=962781, episodic_return=-711.2745318353155\n",
      "global_step=967774, episodic_return=-367.7832740213422\n",
      "global_step=968347, episodic_return=-74.34137931034553\n",
      "global_step=969110, episodic_return=-89.14347826087025\n",
      "global_step=970478, episodic_return=-180.85087108013897\n",
      "global_step=982479, episodic_return=-974.0863787376986\n",
      "global_step=984206, episodic_return=-124.64007782101261\n",
      "global_step=986402, episodic_return=-204.53513513513414\n",
      "global_step=994327, episodic_return=-636.5622950820225\n",
      "global_step=996497, episodic_return=-274.8419580419526\n",
      "global_step=997322, episodic_return=-34.41267605633416\n",
      "global_step=1002233, episodic_return=-524.4664335664615\n",
      "global_step=1004019, episodic_return=-227.80476190475886\n",
      "global_step=1008780, episodic_return=-456.04981273410243\n",
      "global_step=1011880, episodic_return=-281.4981132075351\n",
      "global_step=1023881, episodic_return=-1091.0891089109973\n",
      "global_step=1027614, episodic_return=-406.8979094076672\n",
      "global_step=1028444, episodic_return=-153.8144927536236\n",
      "global_step=1037074, episodic_return=-827.7517684888588\n",
      "global_step=1038105, episodic_return=-136.233333333334\n",
      "global_step=1039284, episodic_return=-189.02616487455117\n",
      "global_step=1040551, episodic_return=-147.1896551724147\n",
      "global_step=1043751, episodic_return=-343.43636363635164\n",
      "global_step=1045221, episodic_return=-160.29480968858223\n",
      "global_step=1046651, episodic_return=-198.77484276729433\n",
      "global_step=1047160, episodic_return=-104.94836601307215\n",
      "global_step=1048791, episodic_return=-236.11428571428232\n",
      "global_step=1049301, episodic_return=-129.82097902097914\n",
      "global_step=1049709, episodic_return=-123.23888888888891\n",
      "global_step=1053023, episodic_return=-309.8885245901562\n",
      "global_step=1065024, episodic_return=-1139.285714285774\n",
      "global_step=1077025, episodic_return=-941.4772727274681\n",
      "global_step=1078379, episodic_return=-199.36938110749048\n",
      "global_step=1079751, episodic_return=-186.49494949494886\n",
      "global_step=1083652, episodic_return=-415.39019607843534\n",
      "global_step=1095653, episodic_return=-631.4381270904331\n",
      "global_step=1098303, episodic_return=-173.31063829787374\n",
      "global_step=1100944, episodic_return=-289.3658385093073\n",
      "global_step=1101172, episodic_return=-103.11948051948046\n",
      "global_step=1102162, episodic_return=-171.7729729729731\n",
      "global_step=1102904, episodic_return=-80.25000000000094\n",
      "global_step=1106729, episodic_return=-353.92162162161156\n",
      "global_step=1107078, episodic_return=-91.53453237410092\n",
      "global_step=1119079, episodic_return=-1126.4214046823442\n",
      "global_step=1131080, episodic_return=-1173.2824427481203\n",
      "global_step=1143081, episodic_return=-1162.3287671233263\n",
      "global_step=1155082, episodic_return=-1181.0126582278695\n",
      "global_step=1167083, episodic_return=-1184.1269841270027\n",
      "global_step=1179084, episodic_return=-1164.6302250804133\n",
      "global_step=1191085, episodic_return=-1178.3783783784036\n",
      "global_step=1202620, episodic_return=-1189.91971830997\n",
      "global_step=1214621, episodic_return=-937.6425855515176\n",
      "global_step=1226622, episodic_return=-1050.364963503788\n",
      "global_step=1228781, episodic_return=-149.61003460207783\n",
      "global_step=1229903, episodic_return=-170.19935691318375\n",
      "global_step=1231936, episodic_return=-231.6714285714255\n",
      "global_step=1243937, episodic_return=-833.5877862596712\n",
      "global_step=1255938, episodic_return=-964.5259938839479\n",
      "global_step=1258408, episodic_return=-272.3319148936117\n",
      "global_step=1270409, episodic_return=-817.100371747391\n",
      "global_step=1282410, episodic_return=-860.1562500001102\n",
      "global_step=1288217, episodic_return=-613.8333333333823\n",
      "global_step=1300218, episodic_return=-929.9578059073124\n",
      "global_step=1312219, episodic_return=-918.5628742516838\n",
      "global_step=1316479, episodic_return=-296.70909090908447\n",
      "global_step=1317077, episodic_return=-91.22393162393183\n",
      "global_step=1318198, episodic_return=-142.36357615894116\n",
      "global_step=1330199, episodic_return=-838.7959866222134\n",
      "global_step=1340566, episodic_return=-1032.122895623038\n",
      "global_step=1347939, episodic_return=-718.4559322034654\n",
      "global_step=1349293, episodic_return=-134.85397923875536\n",
      "global_step=1356058, episodic_return=-379.0874564459721\n",
      "global_step=1356325, episodic_return=-59.83333333333365\n",
      "global_step=1356719, episodic_return=-109.29966777408637\n",
      "global_step=1357192, episodic_return=-126.9657718120806\n",
      "global_step=1358769, episodic_return=-224.53296703296436\n",
      "global_step=1359071, episodic_return=-97.53246753246765\n",
      "global_step=1362018, episodic_return=-261.8835125447995\n",
      "global_step=1364323, episodic_return=-243.68582677165034\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m previous_logprobs \u001b[38;5;241m=\u001b[39m current_logprobs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     70\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 71\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# logging for the losses + learning rate\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "while global_step < TOTAL_TIMESTEPS:\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if LR_SCHEDULING == \"Linear\":\n",
    "        frac = 1.0 - global_step / TOTAL_TIMESTEPS\n",
    "        lrnow = frac * LEARNING_RATE\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, NB_STEPS):\n",
    "        global_step += NB_ENVS\n",
    "        obs[step] = next_obs\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "            current_logprobs[step] = logprob\n",
    "\n",
    "        actions[step] = action\n",
    "\n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "        \n",
    "        if \"episode\" in infos:\n",
    "            completed_episodes = infos[\"_episode\"]\n",
    "            episodic_returns = infos[\"episode\"][\"r\"][completed_episodes]\n",
    "            episodic_lengths = infos[\"episode\"][\"l\"][completed_episodes]\n",
    "\n",
    "            for episodic_return, episodic_length in zip(episodic_returns, episodic_lengths):\n",
    "                print(f\"global_step={global_step}, episodic_return={episodic_return}\")\n",
    "                if RUN_NAME != None:\n",
    "                    writer.add_scalar(\"charts/episodic_return\", episodic_return, global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", episodic_length, global_step)\n",
    "        \n",
    "        # Break when one of the environement as reached a terminal state\n",
    "        if torch.any(next_done):\n",
    "            break\n",
    "    \n",
    "    R = torch.Tensor([0 if next_done[i] == True else values[-1][i] for i in range(len(next_done))]).to(device)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "\n",
    "    for i in reversed(range(step)):\n",
    "        R = rewards[i] + GAMMA * R\n",
    "        returns[i] = R\n",
    "        advantages[i] = returns[i] - values[i]\n",
    "\n",
    "    # Normalize the advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    value_loss = torch.zeros((step, NB_ENVS))\n",
    "    actor_loss = 0\n",
    "    entropy_term = 0\n",
    "\n",
    "    for i in range(step):\n",
    "        _, logprob, ent, value = agent.get_action_and_value(obs[i], actions[i])\n",
    "        value = value.flatten()\n",
    "\n",
    "        actor_loss += -logprob * advantages[i]\n",
    "        value_loss[i] = (returns[i] - value)**2\n",
    "        entropy_term += ent\n",
    "\n",
    "    value_loss = 0.5 * value_loss.mean()\n",
    "\n",
    "    loss = (actor_loss + VF_COEF * value_loss).mean()\n",
    "    approx_kl = (previous_logprobs - current_logprobs).mean() if global_step > NB_ENVS * NB_STEPS else 0.0\n",
    "\n",
    "    previous_logprobs = current_logprobs.detach().clone()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # logging for the losses + learning rate\n",
    "    if RUN_NAME != None:\n",
    "        writer.add_scalar(\"losses/total_loss\", loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/actor_loss\", actor_loss.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", value_loss.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_term.mean().item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl, global_step)\n",
    "        writer.add_scalar(\"charts/learning_rate\", lrnow, global_step)\n",
    "    \n",
    "    \n",
    "# Save the model at the end of training\n",
    "save_path = f\"trained_models/a2c/a2c_config{CONFIG_NUMBER}.pt\"\n",
    "os.makedirs(\"trained_models/a2c\", exist_ok=True)\n",
    "torch.save({\n",
    "    \"model_state_dict\": agent.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"config\": {\n",
    "        \"OBSERVATION_SIZE\": OBSERVATION_SIZE,\n",
    "        \"NB_FRAMES\": NB_FRAMES,\n",
    "        \"NB_ENVS\": NB_ENVS,\n",
    "        \"SEED\": SEED,\n",
    "        \"MAX_EPISODE_LENGTH\": MAX_EPISODE_LENGTH,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"NB_STEPS\": NB_STEPS,\n",
    "        \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,\n",
    "        \"LR_SCHEDULING\": LR_SCHEDULING,\n",
    "        \"GAMMA\": GAMMA,\n",
    "        \"VF_COEF\": VF_COEF\n",
    "    }\n",
    "}, save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "if RUN_NAME != None:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_racing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
