{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e0ce56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf92eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM TP5\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        \n",
    "        obs_dim = int(np.prod(env.observation_space.shape))\n",
    "        \n",
    "        action_dim = env.action_space.shape[0]\n",
    "        \n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(obs_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, action_dim), std=0.01),\n",
    "        )\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def transform_action(self, action):\n",
    "        # Permet de garder les bounds du action space correctes\n",
    "        transformed = action.clone()\n",
    "        # Pour le steering\n",
    "        transformed[..., 0] = torch.tanh(transformed[..., 0])\n",
    "        # Pour le gas et le brake, si présents\n",
    "        if transformed.shape[-1] > 1:\n",
    "            transformed[..., 1:] = torch.sigmoid(transformed[..., 1:])\n",
    "        return transformed\n",
    "        \n",
    "    def get_action(self, obs, action=None):\n",
    "        if obs.dim() > 2:\n",
    "            obs = obs.view(1, -1)\n",
    "\n",
    "        # Calcul de la moyenne\n",
    "        mean = self.actor(obs)\n",
    "        # Calcul de l'écart type (on exponentie log_std)\n",
    "        std = torch.exp(self.log_std).expand_as(mean)\n",
    "\n",
    "        # Création de la distribution Gaussienne \n",
    "        dist = Normal(mean, std)\n",
    "\n",
    "        if action is None:\n",
    "            action = dist.sample()   # Échantillonne une action\n",
    "\n",
    "        # Calcul de la log-probabilité et de l'entropie\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)  # somme sur chaque dimension d'action\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "\n",
    "        transformed_action = self.transform_action(action)\n",
    "\n",
    "        return transformed_action.squeeze(), log_prob, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d75ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space (96, 96, 3)\n",
      "Action space (3,)\n",
      "Observation shape torch.Size([96, 96, 3])\n",
      "New iteration\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCH = 10\n",
    "N_ITERATION = 10\n",
    "N_STEPS = 128\n",
    "N_SAMPLE = 64\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\n",
    "            \"CarRacing-v3\",\n",
    "            continuous=True,\n",
    "            lap_complete_percent=0.95,\n",
    "            domain_randomize=False,\n",
    "            render_mode=\"rgb_array\"\n",
    "        )\n",
    "\n",
    "print(\"Observation space\", env.observation_space.shape)\n",
    "print(\"Action space\", env.action_space.shape)\n",
    "\n",
    "# GRPO algorithm: https://www.youtube.com/watch?v=YCawyzAOg1Y\n",
    "agent = Agent(env).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "actions = torch.zeros(N_SAMPLE, N_STEPS, env.action_space.shape[0]).to(device)\n",
    "log_probs = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "entropies = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "rewards = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "next_observations = torch.zeros((N_SAMPLE, N_STEPS) + env.observation_space.shape, device=device)\n",
    "dones = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "advantages = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "observation, info = env.reset()\n",
    "observation = torch.tensor(observation, dtype=torch.float32).to(device)\n",
    "print(\"Observation shape\", observation.shape)\n",
    "\n",
    "for iter in range(1, N_ITERATION + 1):\n",
    "    print(\"New iteration\")\n",
    "    for step in range(0, N_STEPS):\n",
    "        global_step += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Boucle pour échantillonner N_SAMPLE actions => on a plus le critic model\n",
    "            for i in range(N_SAMPLE):                \n",
    "                action, log_prob, entropy = agent.get_action(observation)\n",
    "                next_observation, reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
    "                done = terminated or truncated\n",
    "\n",
    "                rewards[i, step] = reward\n",
    "                next_observations[i, step] = torch.tensor(next_observation, dtype=torch.float32)\n",
    "                dones[i, step] = done\n",
    "                actions[i, step] = action\n",
    "                log_probs[i, step] = log_prob\n",
    "                entropies[i, step] = entropy\n",
    "\n",
    "        \n",
    "        mean_rewards = rewards[:, step].mean()\n",
    "\n",
    "        std_rewards = rewards[:, step].std() + 1e-8\n",
    "\n",
    "        advantages[:, step] = (rewards[:, step] - mean_rewards) / std_rewards  # advantages formula => Ait = (Ri - mean(R)) / std(R) => paper\n",
    "\n",
    "    break\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlprojet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
