{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ce56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf92eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM TP5\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\"\"\"\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        \n",
    "        obs_dim = int(np.prod(env.observation_space.shape))\n",
    "        \n",
    "        action_dim = env.action_space.shape[0]\n",
    "        \n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(obs_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, action_dim), std=0.01),\n",
    "        )\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def transform_action(self, action):\n",
    "        # Permet de garder les bounds du action space correctes\n",
    "        transformed = action.clone()\n",
    "        # Pour le steering\n",
    "        transformed[..., 0] = torch.tanh(transformed[..., 0])\n",
    "        # Pour le gas et le brake, si présents\n",
    "        if transformed.shape[-1] > 1:\n",
    "            transformed[..., 1:] = torch.sigmoid(transformed[..., 1:])\n",
    "        return transformed\n",
    "    \n",
    "    def get_dist(self, obs):\n",
    "        if obs.dim() > 2:\n",
    "            obs = obs.view(1, -1)\n",
    "\n",
    "        # Calcul de la moyenne\n",
    "        mean = self.actor(obs)\n",
    "        # Calcul de l'écart type (on exponentie log_std)\n",
    "        std = torch.exp(self.log_std).expand_as(mean)\n",
    "\n",
    "        # Création de la distribution Gaussienne \n",
    "        dist = Normal(mean, std)\n",
    "        \n",
    "        return dist\n",
    "        \n",
    "    def get_action(self, obs, action=None):\n",
    "        dist = self.get_dist(obs)\n",
    "\n",
    "        if action is None:\n",
    "            action = dist.sample()   # Échantillonne une action\n",
    "\n",
    "        # Calcul de la log-probabilité et de l'entropie\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)  # somme sur chaque dimension d'action\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "\n",
    "        transformed_action = self.transform_action(action)\n",
    "\n",
    "        return transformed_action.squeeze(), log_prob, entropy\n",
    "\"\"\"  \n",
    "def print_shape(x):\n",
    "    print(x.shape)\n",
    "    return x\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(1, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(64 * 7 * 7, 512)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(512, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        )\n",
    "\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        action_mean = self.actor_mean(x)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6c237",
   "metadata": {},
   "source": [
    "# ANCIEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38dd1f",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d75ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space (96, 96, 3)\n",
      "Action space (3,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction space\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# GRPO algorithm: https://www.youtube.com/watch?v=YCawyzAOg1Y\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m(env)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(agent\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[0;32m     25\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(N_SAMPLE, N_STEPS, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Agent' is not defined"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCH = 10\n",
    "N_ITERATION = 10\n",
    "N_STEPS = 128\n",
    "N_SAMPLE = 64\n",
    "BATCH_SIZE = 64\n",
    "EPSILON = 0.2\n",
    "BETA = 1.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\n",
    "            \"CarRacing-v3\",\n",
    "            continuous=True,\n",
    "            lap_complete_percent=0.95,\n",
    "            domain_randomize=False,\n",
    "            render_mode=\"rgb_array\"\n",
    "        )\n",
    "\n",
    "print(\"Observation space\", env.observation_space.shape)\n",
    "print(\"Action space\", env.action_space.shape)\n",
    "\n",
    "# GRPO algorithm: https://www.youtube.com/watch?v=YCawyzAOg1Y\n",
    "# Inspired from: https://gist.github.com/infoslack/f0e0aec9a882c2c76e1dc1bdd510f279\n",
    "# Inspired from: https://github.com/XinJingHao/DRL-Pytorch/tree/main\n",
    "agent = Agent(env).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "actions = torch.zeros(N_SAMPLE, N_STEPS, env.action_space.shape[0]).to(device)\n",
    "log_probs = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "entropies = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "rewards = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "next_observations = torch.zeros((N_SAMPLE, N_STEPS) + env.observation_space.shape, device=device)\n",
    "dones = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "advantages = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "observation, info = env.reset()\n",
    "observation = torch.tensor(observation, dtype=torch.float32).to(device)\n",
    "print(\"Observation shape\", observation.shape)\n",
    "\n",
    "for iter in range(1, N_ITERATION + 1):\n",
    "    print(\"New iteration\")\n",
    "    for step in range(0, N_STEPS):\n",
    "        global_step += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Boucle pour échantillonner N_SAMPLE actions => on a plus le critic model\n",
    "            for i in range(N_SAMPLE):\n",
    "                action, log_prob, entropy = agent.get_action(observation)\n",
    "                next_observation, reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
    "                done = terminated or truncated\n",
    "\n",
    "                rewards[i, step] = reward\n",
    "                next_observations[i, step] = torch.tensor(next_observation, dtype=torch.float32)\n",
    "                dones[i, step] = done\n",
    "                actions[i, step] = action\n",
    "                log_probs[i, step] = log_prob\n",
    "                entropies[i, step] = entropy\n",
    "\n",
    "        \n",
    "        mean_rewards = rewards[:, step].mean()\n",
    "\n",
    "        std_rewards = rewards[:, step].std() + 1e-8\n",
    "\n",
    "        advantages[:, step] = (rewards[:, step] - mean_rewards) / std_rewards  # advantages formula => Ait = (Ri - mean(R)) / std(R) => paper\n",
    "        break\n",
    "    break\n",
    "\n",
    "    # IL FAUT MTN LOOP SUR LES EPOCHS CALCULER LES RATIO / CLIPRATIO / KL ETC PUIS UPDATE LE MODELE\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0e43a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarRacingRewardPredictor(gym.Wrapper):\n",
    "    env = None\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "    \n",
    "    def create_state_dict(self):\n",
    "        unwrapped_env = self.env.unwrapped\n",
    "\n",
    "        hull = unwrapped_env.car.hull\n",
    "        car_state = {\n",
    "            \"pos\":    hull.position.copy(),\n",
    "            \"angle\":  hull.angle,\n",
    "            \"linvel\": hull.linearVelocity.copy(),\n",
    "            \"angvel\": hull.angularVelocity,\n",
    "        }\n",
    "\n",
    "        wheel_states = []\n",
    "        for wheel in unwrapped_env.car.wheels:\n",
    "            wheel_states.append({\n",
    "                \"pos\":    wheel.position.copy(),\n",
    "                \"angle\":  wheel.angle,\n",
    "                \"linvel\": wheel.linearVelocity.copy(),\n",
    "                \"angvel\": wheel.angularVelocity,\n",
    "                \"omega\":  wheel.omega,\n",
    "                \"phase\":  wheel.phase,\n",
    "                \"tiles\":  set(wheel.tiles),\n",
    "            })\n",
    "\n",
    "        tile_flags = [bool(t.road_visited) for t in unwrapped_env.road]\n",
    "\n",
    "        general_info = {\n",
    "            \"tile_count\":  unwrapped_env.tile_visited_count,\n",
    "            \"cum_reward\":  unwrapped_env.reward,\n",
    "            \"prev_reward\": unwrapped_env.prev_reward,\n",
    "            \"time\":        unwrapped_env.t,\n",
    "        }\n",
    "\n",
    "        return {\"car\": car_state, \"wheels\": wheel_states, \"tiles\": tile_flags, \"general_info\": general_info}\n",
    "\n",
    "    def restore_state(self, state_dict):\n",
    "        unwrapped_env = self.env.unwrapped\n",
    "\n",
    "        hull = unwrapped_env.car.hull\n",
    "        \n",
    "        hull.position = state_dict[\"car\"][\"pos\"]\n",
    "        hull.angle = state_dict[\"car\"][\"angle\"]\n",
    "        hull.linearVelocity = state_dict[\"car\"][\"linvel\"]\n",
    "        hull.angularVelocity = state_dict[\"car\"][\"angvel\"]\n",
    "\n",
    "        for wheel, wheel_state in zip(unwrapped_env.car.wheels, state_dict[\"wheels\"]):\n",
    "            wheel.position = wheel_state[\"pos\"]\n",
    "            wheel.angle = wheel_state[\"angle\"]\n",
    "            wheel.linearVelocity = wheel_state[\"linvel\"]\n",
    "            wheel.angularVelocity = wheel_state[\"angvel\"]\n",
    "            wheel.omega = wheel_state[\"omega\"]\n",
    "            wheel.phase = wheel_state[\"phase\"]\n",
    "            wheel.tiles.clear()\n",
    "            for edge in wheel.contacts:\n",
    "                other_body = edge.other\n",
    "                ud = getattr(other_body, 'userData', None)\n",
    "                if ud and hasattr(ud, 'road_friction'):\n",
    "                    wheel.tiles.add(ud)\n",
    "                    \n",
    "\n",
    "        for tile, tile_flag in zip(unwrapped_env.road, state_dict[\"tiles\"]):\n",
    "            tile.road_visited = tile_flag\n",
    "\n",
    "        unwrapped_env.tile_visited_count = state_dict[\"general_info\"][\"tile_count\"]\n",
    "        unwrapped_env.reward = state_dict[\"general_info\"][\"cum_reward\"]\n",
    "        unwrapped_env.prev_reward = state_dict[\"general_info\"][\"prev_reward\"]\n",
    "        unwrapped_env.t = state_dict[\"general_info\"][\"time\"]\n",
    "    \n",
    "    def compute_single_reward(self, action):\n",
    "        \"\"\"Compute the reward of the given action\n",
    "\n",
    "        Args:\n",
    "            action (np.array): the action to perform\n",
    "\n",
    "        Returns:\n",
    "            float, bool, bool: reward, done flag, truncated flag\n",
    "        \"\"\"\n",
    "        state_dict = self.create_state_dict()\n",
    "        _, reward, done, truncated, _ = self.env.step(action)\n",
    "        self.restore_state(state_dict)\n",
    "        return reward, done, truncated\n",
    "    \n",
    "    def compute_rewards(self, actions):\n",
    "        \"\"\"Given the list of actions, return the associated reward of the given actions\n",
    "\n",
    "        Args:\n",
    "            actions (iterable): actions to perform\n",
    "\n",
    "        Returns:\n",
    "            List[float]: index i contain the reward of the action at index i in actions\n",
    "        \"\"\"\n",
    "        \n",
    "        rewards = []\n",
    "        state_dict = self.create_state_dict()\n",
    "        for action in actions:\n",
    "            _, reward, _, _, _ = self.env.step(action)\n",
    "            self.restore_state(state_dict)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd0ed4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape torch.Size([1, 84, 84])\n",
      "Calculating advantages...\n",
      "Training...\n",
      "Iter 1 / Epoch 1 - Loss π: 1.1160, KL: 1.1160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/autograd/graph.py:823: UserWarning: Error detected in torch::autograd::CopySlices. Traceback of forward call that caused the error:\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_640/1746947157.py\", line 81, in <module>\n",
      "    log_probs[:, step] = logp\n",
      " (Triggered internally at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:122.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 120\u001b[0m\n\u001b[1;32m    118\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    119\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 120\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss π: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, KL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkl_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "from gymnasium.utils.save_video import capped_cubic_video_schedule\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "N_ITERATION   = 10\n",
    "N_STEPS       = 128\n",
    "N_SAMPLE      = 2\n",
    "NUM_EPOCH     = 10\n",
    "EPSILON       = 0.2\n",
    "BETA          = 1.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def make_env(capture_video, run_name):\n",
    "    if capture_video:\n",
    "        env = gym.make(\n",
    "            \"CarRacing-v3\",\n",
    "            continuous=True,\n",
    "            lap_complete_percent=0.95,\n",
    "            domain_randomize=False,\n",
    "            render_mode=\"rgb_array\"\n",
    "        )\n",
    "        env = gym.wrappers.RecordVideo(env, video_folder=f\"videos/{run_name}\", name_prefix=\"eval\",\n",
    "                episode_trigger=capped_cubic_video_schedule)\n",
    "    else:\n",
    "        env = gym.make(\n",
    "        \"CarRacing-v3\",\n",
    "        continuous=True,\n",
    "        lap_complete_percent=0.95,\n",
    "        domain_randomize=False,\n",
    "        render_mode=\"rgb_array\"\n",
    "    )\n",
    "\n",
    "    env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "    env = gym.wrappers.GrayscaleObservation(env)\n",
    "    env = gym.wrappers.FrameStackObservation(env, 1)\n",
    "    env = CarRacingRewardPredictor(env)\n",
    "    \n",
    "    return env\n",
    "\n",
    "env = make_env(False, \"\")\n",
    "\n",
    "\n",
    "# GRPO algorithm: https://www.youtube.com/watch?v=YCawyzAOg1Y\n",
    "# Inspired from: https://gist.github.com/infoslack/f0e0aec9a882c2c76e1dc1bdd510f279\n",
    "# Inspired from: https://github.com/XinJingHao/DRL-Pytorch/tree/main\n",
    "agent      = Agent(env).to(device)\n",
    "policy_old = Agent(env).to(device)\n",
    "policy_old.load_state_dict(agent.state_dict())  # copie initiale\n",
    "optimizer  = optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "next_observations = torch.zeros((N_SAMPLE, N_STEPS) + env.observation_space.shape, device=device)\n",
    "actions      = torch.zeros(N_SAMPLE, N_STEPS, env.action_space.shape[0], device=device)\n",
    "log_probs    = torch.zeros(N_SAMPLE, N_STEPS, device=device)\n",
    "entropies    = torch.zeros(N_SAMPLE, N_STEPS, device=device)\n",
    "rewards      = torch.zeros(N_SAMPLE, N_STEPS, device=device)\n",
    "dones        = torch.zeros(N_SAMPLE, N_STEPS, device=device)\n",
    "advantages   = torch.zeros(N_SAMPLE, N_STEPS, device=device)\n",
    "\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "observation, info = env.reset()\n",
    "observation = torch.tensor(observation, dtype=torch.float32).to(device)\n",
    "print(\"Observation shape\", observation.shape)\n",
    "\n",
    "# --- Boucle principale\n",
    "for iteration in range(1, N_ITERATION + 1):\n",
    "    # 2) Collecte des trajectoires par pas de temps puis épisode\n",
    "    for step in range(N_STEPS):\n",
    "        \"\"\"\n",
    "        DONC ICI IDEALEMENT IL FAUDRAIT FAIRE UNE COPIE DE L'ENVIRONNEMENT\n",
    "        MAIS QUID DU STEP A FAIRE ? => COMMENT LE CHOISIR PARMI LES N_SAMPLES ?\n",
    "        ON POURRAIT PEUT-ETRE REGARDER LA MEILLEURE TRAJECTOIRE ET STEP A LA FIN DE LA BOUCLE N_STEPS\n",
    "        \"\"\"\n",
    "        batch_observation = torch.Tensor(np.array([observation.numpy() for _ in range(N_SAMPLE)]))\n",
    "        action, logp, ent = agent.get_action_and_value(batch_observation)\n",
    "\n",
    "        reward = env.compute_rewards(action.cpu().numpy())\n",
    "\n",
    "        actions[:, step]   = action\n",
    "        log_probs[:, step] = logp\n",
    "        entropies[:, step] = ent\n",
    "        rewards[:, step]   = torch.Tensor(reward)\n",
    "        # next_observations[i, step] = torch.tensor(next_observation, dtype=torch.float32)\n",
    "\n",
    "    # 3) Calcul des avantages normalisés\n",
    "    print(f\"Calculating advantages...\")\n",
    "    for step in range(N_STEPS):\n",
    "        r = rewards[:, step]\n",
    "        advantages[:, step] = (r - r.mean()) / (r.std() + 1e-8)\n",
    "\n",
    "    # 4) Boucle d'apprentissage sur plusieurs époques\n",
    "    print(f\"Training...\")\n",
    "    for epoch in range(1, NUM_EPOCH + 1):\n",
    "        # Recalcul des log-probs sous la policy courante\n",
    "        new_log_probs = []\n",
    "        for i in range(N_SAMPLE):\n",
    "            _ , new_log_probs_item, _  = agent.get_action_and_value(next_observations[i, :], actions[i, :])\n",
    "            new_log_probs.append(new_log_probs_item)\n",
    "\n",
    "        new_log_probs = torch.stack(new_log_probs, dim = 0)\n",
    "\n",
    "        # Calcul du ratio et objectif clipped\n",
    "        ratios = torch.exp(new_log_probs - log_probs)\n",
    "        clipped = torch.clamp(ratios, 1 - EPSILON, 1 + EPSILON)\n",
    "        obj = torch.min(ratios * advantages, clipped * advantages)\n",
    "        \n",
    "        # Pénalité KL\n",
    "        with torch.no_grad():\n",
    "            ratio_kl = torch.exp(log_probs - new_log_probs)\n",
    "        kl_loss = (ratio_kl - torch.log(ratio_kl) - 1).mean()\n",
    "\n",
    "\n",
    "        loss_term = (obj + BETA * kl_loss).mean(dim = 0)\n",
    "\n",
    "        loss = loss_term.mean()\n",
    "        # Mise à jour\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Iter {iteration} / Epoch {epoch} - Loss π: {loss.item():.4f}, KL: {kl_loss.item():.4f}\")\n",
    "\n",
    "    # 5) Mise à jour de la policy de référence\n",
    "    policy_old.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d25b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/10 - loss_pi: -0.0000\n",
      "Iter 2/10 - loss_pi: 0.0000\n",
      "Iter 3/10 - loss_pi: -0.9896\n",
      "Iter 4/10 - loss_pi: -0.9896\n",
      "Iter 5/10 - loss_pi: -0.9896\n",
      "Iter 6/10 - loss_pi: -0.9896\n",
      "Iter 7/10 - loss_pi: -0.9896\n",
      "Iter 8/10 - loss_pi: 0.0000\n",
      "Iter 9/10 - loss_pi: -0.9896\n",
      "Iter 10/10 - loss_pi: -0.9896\n",
      "Training completed in 126.7s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from gymnasium.vector import SyncVectorEnv\n",
    "from gymnasium.wrappers import ResizeObservation, GrayscaleObservation, FrameStackObservation\n",
    "\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "N_ITERATIONS  = 100\n",
    "N_STEPS       = 128\n",
    "G             = 8           # group size (formerly N_SAMPLE)\n",
    "EPSILON       = 0.2\n",
    "BETA          = 1.0\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def make_vec_env():\n",
    "    \"\"\"\n",
    "    Create G parallel CarRacing-v3 envs wrapped with preprocessing and reward predictor.\n",
    "    \"\"\"\n",
    "    def make_one_environment():\n",
    "        env = gym.make(\n",
    "            \"CarRacing-v3\",\n",
    "            continuous=True,\n",
    "            lap_complete_percent=0.95,\n",
    "            domain_randomize=False,\n",
    "            render_mode=\"rgb_array\"\n",
    "        )\n",
    "        # Apply your custom reward predictor wrapper\n",
    "        env = CarRacingRewardPredictor(env)\n",
    "        # Resize, grayscale, and frame-stack as in your original code\n",
    "        env = ResizeObservation(env, (84, 84))\n",
    "        env = GrayscaleObservation(env)\n",
    "        env = FrameStackObservation(env, 1)\n",
    "        return env\n",
    "\n",
    "    return SyncVectorEnv([make_one_environment for _ in range(G)])\n",
    "\n",
    "\n",
    "# Create vectorized environments\n",
    "envs = make_vec_env()\n",
    "obs = envs.reset()[0]                     # returns (obs, infos)\n",
    "obs = torch.tensor(obs, dtype=torch.float32).to(DEVICE)  # shape: (G,84,84)\n",
    "\n",
    "# Initialize agent and old policy\n",
    "agent      = Agent(envs).to(DEVICE)\n",
    "policy_old = Agent(envs).to(DEVICE)\n",
    "policy_old.load_state_dict(agent.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "start_time = time.time()\n",
    "\n",
    "for iteration in range(1, N_ITERATIONS + 1):\n",
    "    # === 1) Collect G trajectories of length N_STEPS ===\n",
    "    all_obs      = []\n",
    "    all_actions  = []\n",
    "    all_logp     = []\n",
    "    all_rewards  = []\n",
    "\n",
    "    for t in range(N_STEPS):\n",
    "        # Sample actions and log-probabilities under current policy\n",
    "        action, logp, _ = agent.get_action_and_value(obs)\n",
    "\n",
    "        next_obs, reward, terminations, truncations, info = envs.step(action.cpu().numpy())\n",
    "        done = np.logical_or(terminations, truncations)\n",
    "        next_obs = torch.tensor(next_obs, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "        all_obs.append(obs)\n",
    "        all_actions.append(action)\n",
    "        all_logp.append(logp)\n",
    "        all_rewards.append(torch.tensor(reward, dtype=torch.float32, device=DEVICE))\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "    # Stack into tensors of shape (G, T, ...)\n",
    "    obs_batch      = torch.stack(all_obs,    dim=1)  # (G, T, obs...)\n",
    "    actions_batch  = torch.stack(all_actions,dim=1)  # (G, T, action_dim)\n",
    "    logp_old_batch = torch.stack(all_logp,   dim=1)  # (G, T)\n",
    "    rewards_batch  = torch.stack(all_rewards,dim=1)  # (G, T)\n",
    "\n",
    "    # === 2) Compute group-relative advantages ===\n",
    "    # 2a) Full returns per trajectory (no discount)\n",
    "    returns = rewards_batch.sum(dim=1)             # (G,)\n",
    "\n",
    "    # 2b) Normalize returns across group\n",
    "    mean_R = returns.mean()\n",
    "    std_R  = returns.std(unbiased=False) + 1e-8\n",
    "    adv    = (returns - mean_R) / std_R            # (G,)\n",
    "\n",
    "    # 2c) Broadcast to shape (G, T)\n",
    "    advantages = adv.unsqueeze(1).expand(-1, N_STEPS)\n",
    "\n",
    "    # === 3) Compute surrogate with integrated forward-KL ===\n",
    "    # Recompute new log-probs under current policy\n",
    "    _, new_logp, _ = agent.get_action_and_value(obs_batch.view(-1, *obs_batch.shape[2:]),\n",
    "                                    actions_batch.view(-1, actions_batch.shape[-1]))\n",
    "    new_logp = new_logp.view(G, N_STEPS)\n",
    "\n",
    "    ratio       = torch.exp(new_logp - logp_old_batch)\n",
    "    clipped_rat = torch.clamp(ratio, 1 - EPSILON, 1 + EPSILON)\n",
    "\n",
    "    # Surrogate terms\n",
    "    sur1 = ratio       * advantages\n",
    "    sur2 = clipped_rat * advantages\n",
    "    # Forward-KL approx per token\n",
    "    kl_per_token = new_logp - logp_old_batch\n",
    "\n",
    "    # Combine surrogate and KL inside the same sum\n",
    "    surrogate_with_kl = torch.min(sur1, sur2) + BETA * kl_per_token\n",
    "\n",
    "    # Actor loss: average first over time, then over group (equivalently .mean())\n",
    "    loss_pi = -surrogate_with_kl.mean()\n",
    "\n",
    "    # === 4) Update policy ===\n",
    "    optimizer.zero_grad()\n",
    "    loss_pi.backward()\n",
    "    optimizer.step()\n",
    "    policy_old.load_state_dict(agent.state_dict())\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Iter {iteration}/{N_ITERATIONS} - loss_pi: {loss_pi.item():.4f}\")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"Training completed in {duration:.1f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_racing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
