{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ce56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf92eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM TP5\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        \n",
    "        obs_dim = int(np.prod(env.observation_space.shape))\n",
    "        \n",
    "        action_dim = env.action_space.shape[0]\n",
    "        \n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(obs_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, action_dim), std=0.01),\n",
    "        )\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def transform_action(self, action):\n",
    "        # Permet de garder les bounds du action space correctes\n",
    "        transformed = action.clone()\n",
    "        # Pour le steering\n",
    "        transformed[..., 0] = torch.tanh(transformed[..., 0])\n",
    "        # Pour le gas et le brake, si présents\n",
    "        if transformed.shape[-1] > 1:\n",
    "            transformed[..., 1:] = torch.sigmoid(transformed[..., 1:])\n",
    "        return transformed\n",
    "    \n",
    "    def get_dist(self, obs):\n",
    "        if obs.dim() > 2:\n",
    "            obs = obs.view(1, -1)\n",
    "\n",
    "        # Calcul de la moyenne\n",
    "        mean = self.actor(obs)\n",
    "        # Calcul de l'écart type (on exponentie log_std)\n",
    "        std = torch.exp(self.log_std).expand_as(mean)\n",
    "\n",
    "        # Création de la distribution Gaussienne \n",
    "        dist = Normal(mean, std)\n",
    "        \n",
    "        return dist\n",
    "        \n",
    "    def get_action(self, obs, action=None):\n",
    "        dist = self.get_dist(obs)\n",
    "\n",
    "        if action is None:\n",
    "            action = dist.sample()   # Échantillonne une action\n",
    "\n",
    "        # Calcul de la log-probabilité et de l'entropie\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)  # somme sur chaque dimension d'action\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "\n",
    "        transformed_action = self.transform_action(action)\n",
    "\n",
    "        return transformed_action.squeeze(), log_prob, entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6c237",
   "metadata": {},
   "source": [
    "# ANCIEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38dd1f",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d75ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space (96, 96, 3)\n",
      "Action space (3,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction space\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# GRPO algorithm: https://www.youtube.com/watch?v=YCawyzAOg1Y\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m(env)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(agent\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[0;32m     25\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(N_SAMPLE, N_STEPS, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Agent' is not defined"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCH = 10\n",
    "N_ITERATION = 10\n",
    "N_STEPS = 128\n",
    "N_SAMPLE = 64\n",
    "BATCH_SIZE = 64\n",
    "EPSILON = 0.2\n",
    "BETA = 1.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\n",
    "            \"CarRacing-v3\",\n",
    "            continuous=True,\n",
    "            lap_complete_percent=0.95,\n",
    "            domain_randomize=False,\n",
    "            render_mode=\"rgb_array\"\n",
    "        )\n",
    "\n",
    "print(\"Observation space\", env.observation_space.shape)\n",
    "print(\"Action space\", env.action_space.shape)\n",
    "\n",
    "# GRPO algorithm: https://www.youtube.com/watch?v=YCawyzAOg1Y\n",
    "# Inspired from: https://gist.github.com/infoslack/f0e0aec9a882c2c76e1dc1bdd510f279\n",
    "# Inspired from: https://github.com/XinJingHao/DRL-Pytorch/tree/main\n",
    "agent = Agent(env).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "actions = torch.zeros(N_SAMPLE, N_STEPS, env.action_space.shape[0]).to(device)\n",
    "log_probs = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "entropies = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "rewards = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "next_observations = torch.zeros((N_SAMPLE, N_STEPS) + env.observation_space.shape, device=device)\n",
    "dones = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "advantages = torch.zeros(N_SAMPLE, N_STEPS).to(device)\n",
    "\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "observation, info = env.reset()\n",
    "observation = torch.tensor(observation, dtype=torch.float32).to(device)\n",
    "print(\"Observation shape\", observation.shape)\n",
    "\n",
    "for iter in range(1, N_ITERATION + 1):\n",
    "    print(\"New iteration\")\n",
    "    for step in range(0, N_STEPS):\n",
    "        global_step += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Boucle pour échantillonner N_SAMPLE actions => on a plus le critic model\n",
    "            for i in range(N_SAMPLE):\n",
    "                action, log_prob, entropy = agent.get_action(observation)\n",
    "                next_observation, reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
    "                done = terminated or truncated\n",
    "\n",
    "                rewards[i, step] = reward\n",
    "                next_observations[i, step] = torch.tensor(next_observation, dtype=torch.float32)\n",
    "                dones[i, step] = done\n",
    "                actions[i, step] = action\n",
    "                log_probs[i, step] = log_prob\n",
    "                entropies[i, step] = entropy\n",
    "\n",
    "        \n",
    "        mean_rewards = rewards[:, step].mean()\n",
    "\n",
    "        std_rewards = rewards[:, step].std() + 1e-8\n",
    "\n",
    "        advantages[:, step] = (rewards[:, step] - mean_rewards) / std_rewards  # advantages formula => Ait = (Ri - mean(R)) / std(R) => paper\n",
    "        break\n",
    "    break\n",
    "\n",
    "    # IL FAUT MTN LOOP SUR LES EPOCHS CALCULER LES RATIO / CLIPRATIO / KL ETC PUIS UPDATE LE MODELE\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ed4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape torch.Size([96, 96, 3])\n",
      "Calculating advantages...\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "N_ITERATION   = 10\n",
    "N_STEPS       = 128\n",
    "N_SAMPLE      = 64\n",
    "NUM_EPOCH     = 10\n",
    "EPSILON       = 0.2\n",
    "BETA          = 1.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\n",
    "    \"CarRacing-v3\",\n",
    "    continuous=True,\n",
    "    lap_complete_percent=0.95,\n",
    "    domain_randomize=False,\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "# GRPO algorithm: https://www.youtube.com/watch?v=YCawyzAOg1Y\n",
    "# Inspired from: https://gist.github.com/infoslack/f0e0aec9a882c2c76e1dc1bdd510f279\n",
    "# Inspired from: https://github.com/XinJingHao/DRL-Pytorch/tree/main\n",
    "agent      = Agent(env).to(device)\n",
    "policy_old = Agent(env).to(device)\n",
    "policy_old.load_state_dict(agent.state_dict())  # copie initiale\n",
    "optimizer  = optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "next_observations = torch.zeros((N_SAMPLE, N_STEPS) + env.observation_space.shape, device=device)\n",
    "actions      = torch.zeros(N_SAMPLE, N_STEPS, env.action_space.shape[0], device=device)\n",
    "log_probs    = torch.zeros(N_SAMPLE, N_STEPS, device=device)\n",
    "entropies    = torch.zeros(N_SAMPLE, N_STEPS, device=device)\n",
    "rewards      = torch.zeros(N_SAMPLE, N_STEPS, device=device)\n",
    "dones        = torch.zeros(N_SAMPLE, N_STEPS, device=device)\n",
    "advantages   = torch.zeros(N_SAMPLE, N_STEPS, device=device)\n",
    "\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "observation, info = env.reset()\n",
    "observation = torch.tensor(observation, dtype=torch.float32).to(device)\n",
    "print(\"Observation shape\", observation.shape)\n",
    "\n",
    "# --- Boucle principale\n",
    "for iteration in range(1, N_ITERATION + 1):\n",
    "    # 2) Collecte des trajectoires par pas de temps puis épisode\n",
    "    for step in range(N_STEPS):\n",
    "        \"\"\"\n",
    "        DONC ICI IDEALEMENT IL FAUDRAIT FAIRE UNE COPIE DE L'ENVIRONNEMENT\n",
    "        MAIS QUID DU STEP A FAIRE ? => COMMENT LE CHOISIR PARMI LES N_SAMPLES ?\n",
    "        ON POURRAIT PEUT-ETRE REGARDER LA MEILLEURE TRAJECTOIRE ET STEP A LA FIN DE LA BOUCLE N_STEPS\n",
    "        \"\"\"\n",
    "        for i in range(N_SAMPLE):\n",
    "            action, logp, ent = agent.get_action(observation)\n",
    "\n",
    "            next_observation, reward, terminated, truncated, _ = env.step(action.cpu().numpy())\n",
    "            done = terminated or truncated\n",
    "\n",
    "            actions[i, step]   = action\n",
    "            log_probs[i, step] = logp\n",
    "            entropies[i, step] = ent\n",
    "            rewards[i, step]   = reward\n",
    "            dones[i, step]     = done\n",
    "            next_observations[i, step] = torch.tensor(next_observation, dtype=torch.float32)\n",
    "\n",
    "    # 3) Calcul des avantages normalisés\n",
    "    print(f\"Calculating advantages...\")\n",
    "    for step in range(N_STEPS):\n",
    "        r = rewards[:, step]\n",
    "        advantages[:, step] = (r - r.mean()) / (r.std() + 1e-8)\n",
    "\n",
    "    # 4) Boucle d'apprentissage sur plusieurs époques\n",
    "    print(f\"Training...\")\n",
    "    for epoch in range(1, NUM_EPOCH + 1):\n",
    "        # Recalcul des log-probs sous la policy courante\n",
    "        new_log_probs = torch.zeros_like(log_probs)\n",
    "        for i in range(N_SAMPLE):\n",
    "            for step in range(N_STEPS):\n",
    "                dist = agent.get_dist(next_observations[i, step])\n",
    "                new_log_probs[i, step] = dist.log_prob(actions[i, step]).sum(dim=-1)\n",
    "\n",
    "        # Calcul du ratio et objectif clipped\n",
    "        ratios = torch.exp(new_log_probs - log_probs)\n",
    "        clipped = torch.clamp(ratios, 1 - EPSILON, 1 + EPSILON)\n",
    "        obj = torch.min(ratios * advantages, clipped * advantages)\n",
    "        loss_pi = -obj.mean()\n",
    "\n",
    "        # Pénalité KL\n",
    "        with torch.no_grad():\n",
    "            ratio_kl = torch.exp(log_probs - new_log_probs)\n",
    "        kl_loss = (ratio_kl - torch.log(ratio_kl) - 1).mean()\n",
    "\n",
    "        # Mise à jour\n",
    "        total_loss = loss_pi + BETA * kl_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Iter {iteration} / Epoch {epoch} - Loss π: {loss_pi.item():.4f}, KL: {kl_loss.item():.4f}\")\n",
    "\n",
    "    # 5) Mise à jour de la policy de référence\n",
    "    policy_old.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04090ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "            \"CarRacing-v3\",\n",
    "            continuous=True,\n",
    "            lap_complete_percent=0.95,\n",
    "            domain_randomize=False,\n",
    "            render_mode=\"rgb_array\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07f65396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gymnasium.wrappers.common.TimeLimit"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e61910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e1d6b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_create_image_array',\n",
       " '_create_track',\n",
       " '_destroy',\n",
       " '_draw_colored_polygon',\n",
       " '_ezpickle_args',\n",
       " '_ezpickle_kwargs',\n",
       " '_init_colors',\n",
       " '_is_protocol',\n",
       " '_np_random',\n",
       " '_np_random_seed',\n",
       " '_reinit_colors',\n",
       " '_render',\n",
       " '_render_indicators',\n",
       " '_render_road',\n",
       " 'action_space',\n",
       " 'bg_color',\n",
       " 'car',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'contactListener_keepref',\n",
       " 'continuous',\n",
       " 'domain_randomize',\n",
       " 'fd_tile',\n",
       " 'get_wrapper_attr',\n",
       " 'grass_color',\n",
       " 'has_wrapper_attr',\n",
       " 'invisible_state_window',\n",
       " 'invisible_video_window',\n",
       " 'isopen',\n",
       " 'lap_complete_percent',\n",
       " 'metadata',\n",
       " 'new_lap',\n",
       " 'np_random',\n",
       " 'np_random_seed',\n",
       " 'observation_space',\n",
       " 'prev_reward',\n",
       " 'render',\n",
       " 'render_mode',\n",
       " 'reset',\n",
       " 'reward',\n",
       " 'road',\n",
       " 'road_color',\n",
       " 'screen',\n",
       " 'set_wrapper_attr',\n",
       " 'spec',\n",
       " 'step',\n",
       " 'surf',\n",
       " 'unwrapped',\n",
       " 'verbose',\n",
       " 'world']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(env.unwrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48549be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "env1 = gym.make(\n",
    "            \"CarRacing-v3\",\n",
    "            continuous=True,\n",
    "            lap_complete_percent=0.95,\n",
    "            domain_randomize=False,\n",
    "            render_mode=\"human\"\n",
    "        )\n",
    "env2 = pickle.loads(pickle.dumps(env1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlprojet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
