{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LA PREMIERE CELL EST POUR TE MONTRER COMMENT LES TRANSFORMATIONS FONCTIONNENT ET COMMENT L'ENVIRONNEMENT EST MODIFIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CarRacing-v3 environment\n",
      "Observation space size: (96, 96, 3)\n",
      "Observation space resize: (64, 64, 3)\n",
      "Observation space: Box(0, 255, (64, 64, 3), uint8)\n",
      "Observation space after gray scaling: Box(0, 255, (64, 64, 1), uint8)\n",
      "Observation space size after frame stacking: Box(0, 255, (64, 64, 1), uint8)\n",
      "Box(0, 255, (64, 64, 2), uint8)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from sb3_contrib import RecurrentPPO  # PPO récurrent (LSTM)\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from gymnasium.wrappers import ResizeObservation, GrayscaleObservation\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# référence pour le dictionnaire d'hyperparamètre et des transformations: https://huggingface.co/sb3/ppo_lstm-CarRacing-v0\n",
    "\n",
    "\"\"\"\n",
    "J'UTILISE DUMMYVECENV ET VECFRAMESTACK CAR LE FrameStackObservation fait bug, chatgpt a proposé cette alternative\n",
    "Voici sa réponse:\n",
    "\"Le problème vient du fait que Stable-Baselines3 (SB3) et ses algorithmes (y compris RecurrentPPO) s'attendent généralement à un environnement \n",
    "vectorisé et utilisent habituellement la classe VecFrameStack pour empiler les observations, plutôt qu'un wrapper Gymnasium standard comme FrameStackObservation\n",
    "\n",
    "SB3 fonctionne de façon optimale avec un VecEnv (environnement vectorisé), par exemple DummyVecEnv ou SubprocVecEnv\n",
    "\n",
    "L'empilement d'images se fait alors via VecFrameStack (fournie par SB3) plutôt qu'un wrapper Gymnasium \n",
    "(qui n'est pas forcément compatible avec la vérification d'espace d'observation qu'effectue SB3)\"\n",
    "\n",
    "\n",
    "=> C'est simplement l'implementation des environements parallèles de stableBaseline (au lieu de celle de gym utilisée dans le TP)\n",
    "\"\"\"\n",
    "\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(\"CarRacing-v3\", continuous=True, render_mode=\"rgb_array\")\n",
    "        env = ResizeObservation(env, (64, 64))\n",
    "        env = GrayscaleObservation(env, keep_dim=True)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "print(\"Loading CarRacing-v3 environment\")\n",
    "env = gym.make(\"CarRacing-v3\", continuous=True, render_mode=\"rgb_array\")\n",
    "print(\"Observation space size:\", env.observation_space.shape)\n",
    "env = ResizeObservation(env, (64, 64)) # 64 from the dict\n",
    "print(\"Observation space resize:\", env.observation_space.shape)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "env = GrayscaleObservation(env, keep_dim = True)\n",
    "print(\"Observation space after gray scaling:\", env.observation_space)\n",
    "\n",
    "n_envs = 8\n",
    "env = DummyVecEnv([make_env() for _ in range(n_envs)])\n",
    "env = VecFrameStack(env, n_stack=2)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflo230702\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Session/Documents/Universite/Master 2/Reinforcement learning/Project/wandb/run-20250405_002021-jw7t69a4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/jw7t69a4' target=\"_blank\">First_full_test</a></strong> to <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Rl2025-project/RL%20Project' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Rl2025-project/RL%20Project/runs/jw7t69a4' target=\"_blank\">https://wandb.ai/Rl2025-project/RL%20Project/runs/jw7t69a4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CarRacing-v3 environment\n",
      "Doing PPOLSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8944bf896c4a7ab48916174074a7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from sb3_contrib import RecurrentPPO  # PPO récurrent (LSTM)\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecNormalize\n",
    "from gymnasium.wrappers import ResizeObservation, GrayscaleObservation\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# référence pour le dictionnaire d'hyperparamètre et des transformations: https://huggingface.co/sb3/ppo_lstm-CarRacing-v0\n",
    "\n",
    "\"\"\"\n",
    "DANS LE DICT IL Y A ENCORE\n",
    "\n",
    "https://rl-baselines3-zoo.readthedocs.io/en/master/guide/config.html\n",
    "\n",
    "('learning_rate', 'lin_1e-4') => decrease linéaire du lr je suppose (cfr TP5)\n",
    "('normalize', \"{'norm_obs': False, 'norm_reward': True}\") => Normalization des reward mais pas des observations\n",
    "('normalize_kwargs', {'norm_obs': False, 'norm_reward': False})] => à investiguer, c'est chelou car par cohérant avec le champ précédent\n",
    "\"\"\"\n",
    "\n",
    "# Dictionnaire policy kwargs venant du dict\n",
    "policy_kwargs = {\n",
    "    \"log_std_init\": -2,\n",
    "    \"ortho_init\": False,\n",
    "    \"enable_critic_lstm\": False,\n",
    "    \"activation_fn\": nn.GELU,\n",
    "    \"lstm_hidden_size\": 128,\n",
    "}\n",
    "\n",
    "# From https://github.com/DLR-RM/rl-baselines3-zoo/blob/325ef5dafe46e483ce9d727d2851aff18b70db7c/rl_zoo3/utils.py#L295\n",
    "def linear_schedule(progress_remaining, init_lr):\n",
    "    return progress_remaining * init_lr\n",
    "\n",
    "# Nombre total de timesteps pour l'entraînement (n_timesteps = 4e6), je pense ça fait référence à n_timesteps dans le dict\n",
    "total_timesteps = 2000000 #4000000.0\n",
    "\n",
    "# Dictionnaire d'hyperparamètres venant du dict\n",
    "hyperparams = {\n",
    "    \"batch_size\": 128,\n",
    "    \"clip_range\": 0.2,\n",
    "    \"ent_coef\": 0.0,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"gamma\": 0.99,\n",
    "    \"learning_rate\": lambda x: linear_schedule(x, 1e-4),\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"n_epochs\": 10,\n",
    "    \"n_steps\": 512,\n",
    "    \"sde_sample_freq\": 4,\n",
    "    \"use_sde\": True,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"policy_kwargs\": policy_kwargs\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=\"Rl2025-project\",\n",
    "    project=\"RL Project\",\n",
    "    name=\"First_full_test\",\n",
    "    config= hyperparams,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    # save_code=True,  # optional\n",
    ")\n",
    "\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(\"CarRacing-v3\", continuous=True, lap_complete_percent=0.95, domain_randomize=False, render_mode=\"rgb_array\") # entrainement sur \"rgb-array\" car plus opti en terme de compute, le mode human display qqch\n",
    "        env = ResizeObservation(env, (64, 64))\n",
    "        env = GrayscaleObservation(env, keep_dim=True)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "print(\"Loading CarRacing-v3 environment\")\n",
    "n_envs = 8\n",
    "env = DummyVecEnv([make_env() for _ in range(n_envs)])\n",
    "env = VecFrameStack(env, n_stack=2)\n",
    "env = VecNormalize(env, norm_reward=True, norm_obs=False)\n",
    "\n",
    "# Instanciation du modèle RecurrentPPO avec la politique 'CnnLstmPolicy'\n",
    "print(\"Doing PPOLSTM\")\n",
    "model = RecurrentPPO(\"CnnLstmPolicy\", env, tensorboard_log=f\"runs/{run.id}\", **hyperparams)\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=total_timesteps,\n",
    "    callback=WandbCallback(\n",
    "        verbose=2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "# Sauvegarde du modèle final\n",
    "model.save(\"q2_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflo230702\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/Session/Documents/Universite/Master 2/Reinforcement learning/Project/wandb/run-20250404_214626-f7vigkl3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/flo230702/RL%20Project/runs/f7vigkl3' target=\"_blank\">pleasant-cosmos-6</a></strong> to <a href='https://wandb.ai/flo230702/RL%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/flo230702/RL%20Project' target=\"_blank\">https://wandb.ai/flo230702/RL%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/flo230702/RL%20Project/runs/f7vigkl3' target=\"_blank\">https://wandb.ai/flo230702/RL%20Project/runs/f7vigkl3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florenthervers/miniconda3/envs/car_racing_env/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to runs/f7vigkl3/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.5     |\n",
      "|    ep_rew_mean     | 22.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 1313     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.9        |\n",
      "|    ep_rew_mean          | 27.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 967         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008340905 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.000483    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.88        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 53          |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 34.2      |\n",
      "|    ep_rew_mean          | 34.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 862       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0099542 |\n",
      "|    clip_fraction        | 0.0689    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.668    |\n",
      "|    explained_variance   | 0.0846    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 16.8      |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.0184   |\n",
      "|    value_loss           | 36.9      |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 44.1         |\n",
      "|    ep_rew_mean          | 44.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 804          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081407465 |\n",
      "|    clip_fraction        | 0.0745       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.643       |\n",
      "|    explained_variance   | 0.229        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25           |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0178      |\n",
      "|    value_loss           | 53.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 57.7        |\n",
      "|    ep_rew_mean          | 57.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 796         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010531253 |\n",
      "|    clip_fraction        | 0.0838      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | 0.391       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 60.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 76.4        |\n",
      "|    ep_rew_mean          | 76.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 762         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008018811 |\n",
      "|    clip_fraction        | 0.0535      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.7        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 67.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 93          |\n",
      "|    ep_rew_mean          | 93          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 740         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004570608 |\n",
      "|    clip_fraction        | 0.0314      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.67        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00798    |\n",
      "|    value_loss           | 43.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 112         |\n",
      "|    ep_rew_mean          | 112         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 730         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006412465 |\n",
      "|    clip_fraction        | 0.0617      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.01        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 39.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 129         |\n",
      "|    ep_rew_mean          | 129         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 710         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004636538 |\n",
      "|    clip_fraction        | 0.0302      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.55       |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18          |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00664    |\n",
      "|    value_loss           | 61.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 146          |\n",
      "|    ep_rew_mean          | 146          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 700          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071300715 |\n",
      "|    clip_fraction        | 0.0564       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.548       |\n",
      "|    explained_variance   | 0.682        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.7         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00601     |\n",
      "|    value_loss           | 43.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 164         |\n",
      "|    ep_rew_mean          | 164         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 704         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002571466 |\n",
      "|    clip_fraction        | 0.021       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.0392      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00494    |\n",
      "|    value_loss           | 53.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 181          |\n",
      "|    ep_rew_mean          | 181          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 697          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027988977 |\n",
      "|    clip_fraction        | 0.0198       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.529       |\n",
      "|    explained_variance   | 0.795        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.4         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00454     |\n",
      "|    value_loss           | 25.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 204         |\n",
      "|    ep_rew_mean          | 204         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 687         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010003166 |\n",
      "|    clip_fraction        | 0.0659      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.222       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    value_loss           | 5.02        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>rollout/ep_len_mean</td><td>▁▁▁▂▂▃▄▄▅▆▆▇█</td></tr><tr><td>rollout/ep_rew_mean</td><td>▁▁▁▂▂▃▄▄▅▆▆▇█</td></tr><tr><td>time/fps</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/approx_kl</td><td>▆▇▆█▆▃▄▃▅▁▁█</td></tr><tr><td>train/clip_fraction</td><td>█▅▅▆▄▂▄▂▄▁▁▅</td></tr><tr><td>train/clip_range</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/entropy_loss</td><td>▁▂▃▄▅▅▅▆▆▇▇█</td></tr><tr><td>train/explained_variance</td><td>▁▂▃▄▄▆▇▅▇▁▇█</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▃▅▇▅▇▃▃▅▅█▅▁</td></tr><tr><td>train/policy_gradient_loss</td><td>▁▁▁▁▅▆▅▆▆▇▇█</td></tr><tr><td>train/value_loss</td><td>▆▅▆▇█▅▅▇▅▆▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>26624</td></tr><tr><td>rollout/ep_len_mean</td><td>203.95</td></tr><tr><td>rollout/ep_rew_mean</td><td>203.95</td></tr><tr><td>time/fps</td><td>687</td></tr><tr><td>train/approx_kl</td><td>0.01</td></tr><tr><td>train/clip_fraction</td><td>0.06587</td></tr><tr><td>train/clip_range</td><td>0.2</td></tr><tr><td>train/entropy_loss</td><td>-0.50097</td></tr><tr><td>train/explained_variance</td><td>0.8627</td></tr><tr><td>train/learning_rate</td><td>0.0003</td></tr><tr><td>train/loss</td><td>0.2216</td></tr><tr><td>train/policy_gradient_loss</td><td>-0.00244</td></tr><tr><td>train/value_loss</td><td>5.0155</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pleasant-cosmos-6</strong> at: <a href='https://wandb.ai/flo230702/RL%20Project/runs/f7vigkl3' target=\"_blank\">https://wandb.ai/flo230702/RL%20Project/runs/f7vigkl3</a><br> View project at: <a href='https://wandb.ai/flo230702/RL%20Project' target=\"_blank\">https://wandb.ai/flo230702/RL%20Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250404_214626-f7vigkl3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy_type\": \"MlpPolicy\",\n",
    "    \"total_timesteps\": 25000,\n",
    "    \"env_name\": \"CartPole-v1\",\n",
    "}\n",
    "run = wandb.init(\n",
    "    project=\"RL Project\",\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    ")\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(config[\"env_name\"])\n",
    "    env = Monitor(env)  # record stats such as returns\n",
    "    return env\n",
    "\n",
    "\n",
    "env = DummyVecEnv([make_env])\n",
    "\n",
    "model = PPO(config[\"policy_type\"], env, verbose=1, tensorboard_log=f\"runs/{run.id}\")\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        verbose=2,\n",
    "    ),\n",
    ")\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_racing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
